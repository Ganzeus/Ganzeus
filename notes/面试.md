## 面试准备

### 自我介绍

我是浙江大学23级的硕士，明年毕业，专业是人工智能，研究方向就是大模型相关的，具体做的是偏底层的推理加速, 主要工作就是写triton算子。这两年主要就是一直在帮导师做他负责的一个工业上国家重点研发项目，整体上比较放养，不是那种研究型的工作，没有写过论文。我对大模型这方面还是挺有兴趣的，不过一直没机会参与过多人的大型项目，除了这个帮导师做项目外就一直在自学。所以我找实习还是希望能多参与到项目中去，积累一下工程经验。



​	I am currently a second-year Master's student at Zhejiang University, majoring in Artificial Intelligence. My research focus is on LLMs, specifically in the area of low-level inference acceleration.

​	I’ve always been interested in large models and AI infrastructure, and I’ve been self-learning a lot in my spare time, experimenting with open-source projects, and trying to stay up to date with what’s happening in the field.

​	Over the past two years, I’ve been working for my Professor on a national industrial project. It's not a research-heavy work, so I didn’t have the chance to publish papers or work in a typical research group, but I did get a lot of experience building and optimizing real systems.

​	Now I’m looking for an internship where I can be more involved in real projects, ideally something collaborative, where I can work with and learn from more experienced engineers and researchers. I’d really love to be part of a team where I can continue growing my technical skills, and contribute to something meaningful.

### 低代码项目介绍

这个项目总的来说就是要做一个低代码平台的工业软件，类似autoML，很多领域比如机械光电航发这些可以用这个软件自动建立和训练机器学习的模型，给工程师推荐工艺参数之类的。我负责的子课题是算子加速这块，就是说这个工业软件需要用到很多的底层算子，矩阵相乘、卷积等等，需要通过各种方法提升算子的运行效率，最后再和一个叫SAS的数据分析软件做对比测试，算子性能比SAS快50%就达到了指标。我做的就是用triton重写了这些算子，包括矩阵相乘、卷积、注意力这些kernel，结合了量化、算子融合等方法。

这个项目其实是相当于是我一个人探索摸索出来的，因为当时刚参与到这个项目的时候，这个算子加速的课题之前师兄还没有开始做，导师是让我一个人搞这个子课题的，而且这个子课题比较独立，因为大课题就是一个偏理论研究的内容，所以什么数据集模型都没有，我花了好几周才拿到一个数据集上手搭模型开始跑代码的。



比如矩阵相乘算子就是一个int8量化版本的，速度是PyTorch的矩阵相乘的两倍；还有卷积算子用的是implicit gemm的方法，就是把卷积转换为矩阵相乘；attention算子就是用triton实现的flash attention嘛，也是有int8量化版本的。当然这些算子都是先在github上搬过来的，先把它跑起来，再慢慢啃，理解了之后再做一些修改。有了算子之后就在这个基础上写了对应的网络层，conv2d、linear这样的和PyTorch api类似的网络层，最后再建立模型跑推理，测试推理性能。跟SAS对比每个层的推理耗时，基本上干的事就是这么多。



### 遇到的挑战

最大挑战：跟SAS的对比测试

SAS算子闭源，不能单独调用具体算子，最细就只能一层一层的推理，所以要和SAS算子对比的话必须在模型层面比推理耗时，就是要搭建一个模型，然后测试模型每一层的推理耗时，把一个层当作一个算子，再和triton对比。如果和PyTorch原生算子对比的话就简单多了，triton有官方的benchmark流程，还有tensorboard profiler可以比较更细的GPU占用率，整个推理过程的算子调用顺序等等。



说到挑战的话，其实一开始接触这个项目，一整个项目对我来说都是个非常大的挑战，当时刚进来根本不知道干什么，就干了干整个项目的任务书，任务书写的特别抽象，全是各种大话，就和那些党中央领导讲话一样，完全没有具体的任务安排，而且算子加速这个子任务就完全交给我一个人，之前这个任务根本没开始，所以全得靠我自己体会。一开始导师告诉我跑一个模型的加速，但连数据集都没有，模型更是让我随便找，所以我就特别迷，每天在漫无目的的看一些推理加速的论文博客，什么量化、算子融合之类的。后来几个星期之后其他课题那边有了一个数据集，到我手里之后我才开始搭了一个模型上手跑起来。所以我找实习也是想找一个有指导的，能快速参与到项目最好。后来24年我导师发现triton开始火起来了，就让我也看看。当时我对GPU啊、并行计算之类是完全不了解，我花了好长时间才基本入门，看了一本cuda并行编程的书，又看了triton的官方文档，后来才逐渐学明白了，自己也动手写了一些kernel。说会遇到的挑战，我觉得遇到的最大的挑战就是关于转置算子，因为任务书有个指标就是转置算子的加速，虽然实现triton转置算子很简单，但是最后要和SAS对比的话必须得放在模型上跑（SAS是闭源的，不能直接调用底层算子，只能测出来整个一层的推理耗时），但是深度学习又没有什么直接运用转置的层，当时就困扰了很长时间，我感觉就是写任务书的人也不懂，随便写了个转置算子上去，最后麻烦的只有我一个人。后来我就索性用attention kernel替代转置算子了，跟导师讨论了他也觉得没问题，就这么干了。所以最大的挑战就是这个吧




## 面试记录

| Date                          | 记录                                                         | 感受                                                         | 结果       |
| ----------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ---------- |
| 5/19 20：00 字节AI Infra      | 问了triton相关细节，手撕代码没过                             | 第一次面试无经验，感觉被拷打                                 | 无后续     |
| 5/29 11：00 蚂蚁 导师推荐     | 电话面试，问了项目相关和研究生相关经历                       | 面试官有点condescending                                      | 性格测试挂 |
| 6/5 19：00 海康威视           | 面试官像个大学老师，一直在问我到底为什么对大模型感兴趣       | 感觉和考研复试面试差不多                                     | 挂         |
| 6/6 14：00 要务科技 机械星球  | 纯聊天，聊简历                                               | 没什么印象                                                   |            |
| 6/9 9：50 上海刑事科学技术院  | 自我介绍+项目，问我有没有LLM应用和微调方面的经历             | 方向不太匹配                                                 |            |
| 6/9 19：00 美团北斗           | 自我介绍+项目拷打，问的比较深，还让我现场写kernel，还问了个c++的结构体 | 感觉面试官挺专业，估计没有二面了，自身实力不够               |            |
| 6/10 博世多模态               | 英文自我介绍+问了多模态相关问题                              | 岗位根本不匹配，啥也没问就结束了                             |            |
| 6/11 京东 后端开发岗 北京     | 自我介绍+现场写softmax算子+问几个推理框架+编译器问题全都不会 | 面试官很专业，我啥也不会                                     |            |
| 6/16 18:00 快手大模型训推优化 | 自我介绍+经历提问+MLA解释+layernorm kernel + 算法题          | 面试过程很轻松自由，写代码的时候面试官在干其他事             |            |
| 6/18 10：10 南京某国企 夏令营 | 自我介绍+微调和RAG区别+项目介绍                              | 没说完疯狂打断，面试体验很差，逆天面试官不尊重人，还问导师是不是教授，为什么不考南大 |            |
| 6/23 15：00 字节 搜索         | l2 cache怎么提高的命中率+triton底层+C++ struct内存占用+ 树后序遍历递归+非递归 | 问的问题非常细，                                             |            |
|                               |                                                              |                                                              |            |
|                               |                                                              |                                                              |            |
|                               |                                                              |                                                              |            |
|                               |                                                              |                                                              |            |
|                               |                                                              |                                                              |            |
|                               |                                                              |                                                              |            |
|                               |                                                              |                                                              |            |
|                               |                                                              |                                                              |            |
|                               |                                                              |                                                              |            |





## triton相关问题整理

### Triton 编译器架构与设计

- **Q1: Triton 编译器的整体架构如何？**
   **答：** Triton 采用基于 LLVM 的模块化编译架构。使用 `@triton.jit` 装饰 Python 函数时，编译器会对函数的 AST 进行解析，生成一种称为 Triton-IR 的中间表示（IR），其中多维数据块（tiles）是一级公民。Triton-IR 是一种未优化、机器无关的中间表示。之后，编译器对 Triton-IR 进行优化（如自动划分并行任务、优化内存访问、张量乘加融合等），生成 Triton-GPUIR（TTGIR），再通过 LLVM 后端生成最终的 PTX 代码。简而言之，Triton 将 Python kernel 降低为多层 IR，再经过 LLVM 生成高效的 GPU 汇编。

- **Q2: Triton-IR 有哪些特点？**
   **答：** Triton-IR 与传统 LLVM IR 不同，它以**块级（tile-level）**运算为基础，直接支持向量和矩阵运算。编译器可以基于 Triton-IR 自动完成许多优化任务：例如，识别出计算密集的块级操作（如 `tl.dot` 等），自动将数据缓存到共享内存，并通过活跃度分析决定何时同步。在并行度方面，Triton 能自动将不同 kernel 实例分配到不同 SM（工作组级并行），并在单个 SM 内根据迭代空间自动划分工作到各 warp/线程。另外，Triton-IR 和 Triton-GPUIR 都基于 MLIR（LLVM 项目的子项目），使得 Triton 能利用 LLVM 生态的优化算法。

- **Q3: Triton 如何支持 Kernel Fusion？**
   **答：** Triton 鼓励在单个 kernel 中融合多步运算，以减少内存读写。例如，在实现 Softmax 时，可以在一个 kernel 内依次完成求最大值、指数化、归一化等步骤，而不把中间结果写回显存。这样可以避免多次 DRAM 读写，大大提高带宽利用率。Triton 框架提供丰富的张量操作，用户直接在一个 kernel 中调用多种运算即可实现融合。相关示例表明：此类融合往往能带来几倍的加速（例如矩阵乘加 + ReLU 的融合相较两次分开运算可接近 2× 性能提升）。Triton 自身并不自动融合不同 kernel，融合需要在编写 kernel 时由开发者完成，但编译器可对融合后的代码进行优化。

- **Q4: Triton 有哪些自动调优 (autotuning) 机制？如何使用？**
   **答：** Triton 提供 `@triton.autotune` 装饰器让用户指定多组可能的配置，框架会在运行时测试这些配置并选出最优。用户传入一系列 `triton.Config`（指定不同的 BLOCK_SIZE、num_warps 等）和调优键（key），当相应参数变化时，所有配置都会被评测。例如，可以写：

  ```python
  @triton.autotune(configs=[
      triton.Config(kwargs={'BLOCK_SIZE': 128}, num_warps=4),
      triton.Config(kwargs={'BLOCK_SIZE': 1024}, num_warps=8),
    ], key=['N'])
  @triton.jit
  def kernel(..., N, BLOCK_SIZE: tl.constexpr):
      ...
  ```

  在首次运行时，Triton 会运行并比较各配置耗时，选出最快的。通过设置环境变量 `TRITON_PRINT_AUTOTUNING=1`，还可以让 Triton 在终端打印每个 kernel 自动调优的结果（包括耗时和最佳配置）。在实际工程中，可将调优结果缓存到文件或数据库（类似 XLA 的持久化缓存机制），以便在后续相同情形下直接加载加速启动。

- **Q5: Triton Kernel 的编写和调试方法有哪些？**
   **答：** 编写 Triton Kernel 就像写 Python 函数：在函数前加 `@triton.jit` 装饰，用 `triton.language` 提供的张量操作 (`tl.load`, `tl.store`, `tl.dot`, `tl.exp` 等) 来构造计算。调试方面，Triton 提供四个内置操作符：`static_print`/`static_assert`（用于编译时检查）和 `device_print`/`device_assert`（用于运行时检查，仅在设置 `TRITON_DEBUG=1` 时生效）。此外，还可以用 CPU 解释模式来调试：设置环境变量 `TRITON_INTERPRET=1`，使 Triton 在 CPU 上用 NumPy 逐步模拟执行 kernel，方便打印和单步调试。对于 GPU 上的内存访问错误，也可以使用 NVIDIA 的 compute-sanitizer 等工具进行动态检测。

### Triton 与 CUDA 的对比

- **Q6: Triton 相对 CUDA 的主要优点是什么？**
   **答：** Triton 最大的优势在于更高层次的抽象和易用性。开发者可以用 Python 编写 kernel，免去了 CUDA C++ 的冗长模板和错误陷阱。Triton 编译器会自动处理许多低级细节，比如自动对齐内存访问和共享内存管理，显著降低了编程复杂度。此外，Triton 在一个 kernel 中融合多操作（如 Softmax 的多步运算）也十分方便，能充分利用硬件带宽。
- **Q7: Triton 相对 CUDA 的缺点或局限有哪些？**
   **答：** Triton 目前主要针对 NVIDIA GPU 设计，不支持其他设备。与 CUDA 相比，Triton 在跨 SM 的调度上仍然需要用户手动指定 kernel 网格（grid）大小。如果需要非常精细地控制线程拓扑或使用最新硬件特性（例如直接使用新指令集、特殊缓存策略等），CUDA 提供的底层 API 会更灵活。CUDA 生态成熟，已有大量经过深度优化的库（如 cuBLAS、cuDNN、CUTLASS 等），某些标准算子使用这些库可能比自己写 Triton kernel 更方便、高效。此外，Triton 的 JIT 编译也意味着启动开销（首次运行时编译）会比静态 CUDA 稍大，对于极小的、频繁启动的 kernel 可能影响性能。
- **Q8: Triton 与 CUDA 在性能调优上的差异？**
   **答：** 在性能上，Triton 生成的 kernel 往往与手写 CUDA 相当，尤其在算子融合和宽度匹配场景下表现良好。调优思路上，Triton 用户仍需考虑线程块大小（BLOCK_SIZE）、warp 数量等对带宽和计算的影响，也需避免 warp 分歧等问题。这些与 CUDA 调优类似。不同的是，Triton 会自动合并内存访问和共享内存使用，开发者可以少操心这些低级优化，但仍可手动调 `num_stages`、`num_warps` 等参数。性能调优时，可以使用 NVIDIA 的 Nsight Compute 等工具分析 kernel 的瓶颈（如内存访问是否合并、L2 缓存命中率等），还可以利用 `TRITON_PRINT_AUTOTUNING` 输出的日志来指导块大小选择。

### Triton vs CUDA 关键对比

| 特性                      | CUDA (手动)                          | Triton (自动)                                           |
| ------------------------- | ------------------------------------ | ------------------------------------------------------- |
| **内存对齐 (Coalescing)** | 手动优化（需要保证访问模式）         | 自动合并访问                                            |
| **共享内存管理**          | 手动分配/同步                        | 编译器自动识别、缓冲数据                                |
| **SM 内部线程调度**       | 手动分配线程块给 SM、设定 block 大小 | 编译器自动并行化、在 SIMD 单元中分配任务                |
| **SM 间线程调度**         | 手动设定 grid 大小                   | 手动（Triton 不跨 SM 自动分配）                         |
| **编程模型**              | C/C++ (CUDA API)                     | Python API (`@triton.jit` 函数)                         |
| **调试工具**              | cuda-gdb、printf                     | 内置 `static_print/device_print`、`TRITON_INTERPRET` 等 |
| **现有库支持**            | 丰富（cuBLAS, cuDNN 等）             | 主要依赖自定义 kernel 开发，库支持尚少                  |



### GPU 并行计算基础

- **Q9: CUDA 的线程模型：warp、thread、block 有何区别？**
   **答：** 在 CUDA 中，每个多处理器（SM）同时调度执行一个 warp 的指令，当前架构下一个 warp 包含 32 个线程。线程（thread）是最小执行单元，多个线程组成线程块（block），多个线程块组成网格（grid）。程序中通过配置每个 kernel 的 block 大小（每个 block 包含多少线程）和 grid 大小来控制并行度。一个线程块内的线程共享同一块 SRAM（共享内存），线程块之间可以看作无直接同步（除非使用全局同步原语）。概括：每个 block 有若干 warp，warp 内的 32 个线程同时执行同一条指令；多个 block 在多个 SM 上并发执行。
- **Q10: GPU 的内存层次结构有哪些？**
   **答：** CUDA 的内存层次从近到远依次是：寄存器 → 共享内存/L1 缓存 → L2 缓存 → 全局内存（显存）→ 主机内存。每个线程有私有的寄存器和 local 内存，每个线程块有共享内存供所有线程快速访问。全局内存对所有线程可见，但访问延迟最高。常量内存和纹理内存是只读缓存，优化特定的访问模式。此外，现代 NVIDIA GPU 有各层高速缓存（如 Ampere 的 L2 缓存），用以提高全局访问效率。调优时，应尽量多用共享内存和 L1 缓存来复用数据，减少对全局内存的直接访问。
- **Q11: 什么是内存合并 (coalescing)？如何避免冲突？**
   **答：** 内存合并指的是当同一个 warp 的线程同时访问相邻地址时，硬件会将多个内存访问合并为少量传输。例如在 Volta 及以后架构上，一个 warp 可以以 128 字节（32×4-byte）的块为单位访问，只要每个线程访问连续地址且对齐，就能打包为一次传输。反之，如果访问不对齐或散乱，就可能产生多个事务，降低带宽利用率。在 Triton 中，使用 `tl.arange` 等生成连续地址序列可以自动获得合并访问。另外，共享内存中也要避免**bank 冲突**，即同一时刻过多线程访问同一个 bank（16/32 位交错的片段）；编写代码时可通过添加填充或调整访问模式来消除冲突。
- **Q12: 什么是占用率 (Occupancy)？为什么重要？**
   **答：** 占用率定义为每个 SM 上**活跃 warp 数**与该 SM 最大活跃 warp 数的比值。换言之，若 SM 最多可并行 64 个 warp，当活跃的 warp 只有 32 个时，占用率为 50%。高占用率意味着有更多并发线程可执行，可以更好地隐藏指令和内存的延迟；但过高占用率也可能导致每线程资源减少（如寄存器变少，产生 spill），从而降低性能。通常用 CUDA 提供的 Occupancy API（如 `cudaOccupancyMaxActiveBlocksPerMultiprocessor`）或 Nsight 工具来评估占用率。在调优时，若发现占用率过低，可增大 block 大小或减少每线程使用的资源；若占用率已满，还需关注是否有其他瓶颈（如内存带宽）。


### 实际项目经验与性能分析

- **Q13: 如何在训练/推理流水线中集成自定义 Triton Kernel？**
   **答：** 在 PyTorch 中，可以直接在前向/后向函数里调用 Triton kernel，就像调用普通的 tensor 操作。例如，定义 Triton kernel 后，在训练循环中用 `add_kernel[grid](args)` 这样启动。对于 PyTorch 2.x，还可通过 `torch.compile` 将 Triton kernel 与模型编译到一起。PyTorch 官方教程展示了如何在 `torch.compile` 环境下嵌入 Triton 向量加法 kernel，实现了将 Triton 优化计算加入到模型中的流程。在更高层面，可以将 Triton kernel 封装为自定义算子供前端调用，或借助 TorchDynamo/Inductor 等工具自动识别并转换。总体思路是：确保输入输出是 CUDA tensor，然后像使用其他 CUDA 运算那样调用 Triton kernel 即可无缝集成。
- **Q14: 如何对 Triton Kernel 进行性能分析（Profiling）？**
   **答：** Triton kernel 其实是 CUDA kernel，所以可以使用 NVIDIA 提供的各种工具进行分析。常用的方法包括：使用 `nvprof`（旧版已弃用）、NVIDIA Nsight Compute、Nsight Systems 等采集 kernel 级别的指标。这些工具可以测量每个 kernel 的执行时间、内存吞吐、warp 效率、指标带宽等。使用 Triton 时，也可以用 `TRITON_PRINT_PERF_COUNTERS` 环境变量让 Triton 在运行时输出一些统计信息。针对 PyTorch 模型级的瓶颈，则可用 PyTorch Profiler（`torch.profiler`）结合 TensorBoard 可视化来定位是哪个操作耗时最多。分析时应关注是否有内存合并问题、寄存器溢出、分支分歧等因素。
- **Q15: 进行性能分析时应关注哪些指标？**
   **答：** 常见关键指标包括：**内存带宽利用率**（看全局/共享内存访问是否为瓶颈）、**SM 利用率**（活跃 warp 数，反映占用率）、**warp 效率**（包括分支分歧情况）、**寄存器和共享内存占用**（高占用可能导致并行度下降）、以及 **Tensor 核心利用率**（若算子可用 Tensor Core 应检查是否被充分利用）。例如，要检查全局内存访问是否合并，可看“global load throughput”是否接近理论带宽；若性能低，可检查合并率（nsight 中常有“global_transactions_per_request”指标）。其他如延迟（时序）、L2 缓存命中率等也值得关注。综合这些指标，判断是计算瓶颈还是内存瓶颈，据此修改 kernel 参数或算法。



### 系统设计相关问题

- **Q16: 如何设计 Kernel Autotuning 系统？**
   **答：** Kernel Autotuning 系统需要自动选择最佳的运行参数以适配不同的输入规模和硬件。一般思路是预先定义若干参数组合（例如不同的 BLOCK_SIZE、num_warps、pipeline 阶段数等），然后通过基准测试评估其性能。Triton 本身提供了简单的 `@triton.autotune` 接口，用户给出配置列表，框架运行并自动记录最快配置。在大规模系统中，可把这些调优结果持久化：如 XLA 会为每个图计算**融合子图**创建调优缓存。一个完整的 autotuner 可能需要多轮搜索：首先通过粗粒度搜索找到候选参数，再用性能模型或细粒度测试做剪枝优化。同时也要考虑调优时间成本，在部署时尽量复用已有结果（例如多模型共享同一缓存目录）来加速。
- **Q17: 如何进行异构调度设计？**
   **答：** 异构调度涉及在混合资源（如多 GPU、GPU+CPU、甚至 FPGA/TPU）上分配任务。设计时首先需要对不同资源的性能做建模，比如估算同一任务在 GPU 和 CPU 上的执行时间。可以采用集中式调度器或分布式调度框架，例如 Kubernetes + GPU 分配（Node 可附带 GPU），或 ML 专用调度器（如 Ray、KubeFlow）来实现资源管理。针对深度学习推理任务，常用方案是将模型分发到不同 GPU 或机器，利用异步流水线并行处理多个输入；对于训练则有数据并行、模型并行等策略。需要处理负载均衡、通信开销和数据依赖等问题。在面试中，可以讨论如何通过 Batch、流水线并行或 Multi-Process Service (MPS) 等技术来提高 GPU 利用率，以及当资源过载时如何自动扩缩容。
- **Q18: 常见的模型编译框架有哪些？**
   **答：** 业界主流的 AI 模型编译/优化框架包括：Google 的 XLA（用于 TensorFlow/JAX，做图融合和低层代码生成）；Apache TVM（为各种硬件生成高效代码，支持自动调度）；Meta 的 Glow（将高层模型编译到多种硬件）；NVIDIA TensorRT（针对 NVIDIA GPU 的高性能推理引擎）；ONNX Runtime（多后端推理）；以及 PyTorch 的 nvFuser/Inductor（用于动态图自动融合和优化）。这些框架大多基于 MLIR 或类似中间表示技术，能自动执行算子融合、内存布局优化、代码生成等。例如，将两个常见操作（矩阵乘 + 激活）编译成一个 fused kernel 就是 XLA/TVM 的基本功能。根据场景不同，可能选用不同框架：研究原型用灵活性高的 TVM，在线推理用优化程度极高的 TensorRT 等。
- **Q19: 部署流水线的关键组成和优化思路？**
   **答：** 部署流水线通常包括：模型导出（如 ONNX/TensorRT 解析）、镜像化部署、在线推理服务、监控和扩容策略。常用的推理服务框架有 NVIDIA Triton Inference Server、TensorFlow Serving、TorchServe 等。以 NVIDIA Triton 为例，它支持多种后端模型格式，并提供负载均衡、并行批处理等功能。部署时需配置好硬件（选择合适 GPU 型号、多 GPU 并行部署）和软件参数（batch 大小、并发流数等）。NVIDIA 还提供专门的调优工具：**Perf Analyzer** 用于测试模型吞吐和延迟；**Model Analyzer** 可以自动或手动搜索不同配置组合（例如 batch 大小、并发请求数）以找到最佳性能设置。生产环境中，还要考虑 A/B 测试、日志监控和冷启动预热等。总之，优化思路是通过工具获取基准数据，根据瓶颈调整配置，并利用自动化手段（如模型仓库、CI/CD）持续集成和升级模型部署。





## 八股笔记

### 数据并行DP/流水线并行PP/张量并行TP

| DP                  | PP                                                  | TP                      |
| ------------------- | --------------------------------------------------- | ----------------------- |
| 切分数据            | 切分模型                                            | 切分矩阵                |
| 每个GPU保存完整模型 | 每个GPU只有模型的一部分                             | 就是cuda/triton并行计算 |
|                     | 进一步划分micro batch,按流水线方式分别给每个GPU处理 |                         |

DP+PP:

+ 宏观数据并行
+ 微观流水线并行

ZeRO DP:

+ ZeRO: Zero Redundancy Optimizer
+ 用于解决DP的缺点：浪费显存（每个GPU都存放模型+梯度+optimizer state）
+ 方法：切分整个模型的参数，每个GPU只存放一部分参数，计算时需要所有GPU先通信，得到完整模型再计算
+ **切分逻辑和流水线并行不同：**
  + ZeRO DP是横向切：切的是每一层的参数，计算时需要GPU通信
  + PP是纵向切，切的是模型的层，不需要GPU通信





### Optimizer

#### SGD

SGD（随机梯度下降）和原始的梯度下降的主要区别是：**每次更新参数时用的数据量不同**。

+ SGD每次用一个batch的梯度更新网络
+ 而原始GD必须把整个数据集梯度全部算好后再更新网络



#### momentum

普通梯度下降：
```
参数 = 参数 - 学习率 × 当前梯度
```

加了动量的梯度下降

```
速度 = 衰减率 × 之前的速度 - 学习率 × 当前梯度
参数 = 参数 + 速度
```



#### SwigLU





### vLLM

#### PagedAttention

> KV Cache 被划分为块。块在内存空间中不需要连续。

​	由于块blocks在内存中不需要连续，因此我们可以像在操作系统的虚拟内存中一样以更灵活的方式管理键和值keys & values：可以将块视为页面pages，将tokens视为字节bytes，将序列sequences视为进程processes。序列的连续 ***逻辑块*** ***logical blocks*** 通过 **块表** **block table** 映射到非连续 **物理块** ***physical blocks***。物理块在生成新tokens时按需分配。
