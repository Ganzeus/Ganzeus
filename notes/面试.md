## 面试准备

### 自我介绍

我是浙江大学23级的硕士，明年毕业，专业是人工智能，研究方向就是大模型相关的，具体做的是偏底层的推理加速, 主要工作就是写triton算子。这两年主要就是一直在帮导师做他负责的一个工业上国家重点研发项目，基本上属于是一直在自学，比较放养的那种，没有写过论文。我对大模型主要了解就是推理相关的，相对底层的这块，至于训练啊agent之类的就没什么经验，所以我倾向于那种推理优化的岗位。



​	I am currently a second-year Master's student at Zhejiang University, majoring in Artificial Intelligence. My research focus is on LLMs, specifically in the area of low-level inference acceleration.

​	I’ve always been interested in large models and AI infrastructure, and I’ve been self-learning a lot in my spare time, experimenting with open-source projects, and trying to stay up to date with what’s happening in the field.

​	Over the past two years, I’ve been working for my Professor on a national industrial project. It's not a research-heavy work, so I didn’t have the chance to publish papers or work in a typical research group, but I did get a lot of experience building and optimizing real systems.

​	Now I’m looking for an internship where I can be more involved in real projects, ideally something collaborative, where I can work with and learn from more experienced engineers and researchers. I’d really love to be part of a team where I can continue growing my technical skills, and contribute to something meaningful.

### 低代码项目介绍

这个项目总的来说就是要做一个低代码平台的工业软件，类似autoML，很多领域比如机械光电航发这些可以用这个软件自动建立和训练机器学习的模型，给工程师推荐工艺参数之类的。我负责的子课题是算子加速这块，就是说这个工业软件需要用到很多的底层算子，矩阵相乘、卷积等等，需要通过各种方法提升算子的运行效率，最后再和一个叫SAS的数据分析软件做对比测试，算子性能比SAS快50%就达到了指标。我做的就是用triton重写了这些算子，包括矩阵相乘、卷积、注意力这些kernel，结合了量化、算子融合等方法。

这个项目相当于是我一个人摸索出来的，因为当时刚参与到这个项目的时候，这个算子加速的课题之前师兄还没有开始做，导师是让我一个人搞这个子课题的，而且这个子课题比较独立，因为大课题就是一个偏理论研究的内容，一开始什么数据集模型都没有，我花了好几周才拿到一个数据集上手搭模型开始跑代码的。



比如矩阵相乘算子就是一个int8量化版本的，速度是PyTorch的矩阵相乘的两倍；还有卷积算子用的是implicit gemm的方法，就是把卷积转换为矩阵相乘；attention算子就是用triton实现的flash attention嘛，也是有int8量化版本的。当然这些算子都是先在github上搬过来的，先把它跑起来，再慢慢啃，理解了之后再做一些修改。有了算子之后就在这个基础上写了对应的网络层，conv2d、linear这样的和PyTorch api类似的网络层，最后再建立模型跑推理，测试推理性能。跟SAS对比每个层的推理耗时，基本上干的事就是这么多。



### 遇到的挑战

最大挑战：跟SAS的对比测试

SAS算子闭源，不能单独调用具体算子，最细就只能一层一层的推理，所以要和SAS算子对比的话必须在模型层面比推理耗时，就是要搭建一个模型，然后测试模型每一层的推理耗时，把一个层当作一个算子，再和triton对比。如果和PyTorch原生算子对比的话就简单多了，triton有官方的benchmark流程，还有tensorboard profiler可以比较更细的GPU占用率，整个推理过程的算子调用顺序等等。



说到挑战的话，其实一开始接触这个项目，一整个项目对我来说都是个非常大的挑战，当时刚进来根本不知道干什么，就干了干整个项目的任务书，任务书写的特别抽象，全是各种大话，就和那些党中央领导讲话一样，完全没有具体的任务安排，而且算子加速这个子任务就完全交给我一个人，之前这个任务根本没开始，所以全得靠我自己体会。一开始导师告诉我跑一个模型的加速，但连数据集都没有，模型更是让我随便找，所以我就特别迷，每天在漫无目的的看一些推理加速的论文博客，什么量化、算子融合之类的。后来几个星期之后其他课题那边有了一个数据集，到我手里之后我才开始搭了一个模型上手跑起来。所以我找实习也是想找一个有指导的，能快速参与到项目最好。后来24年我导师发现triton开始火起来了，就让我也看看。当时我对GPU啊、并行计算之类是完全不了解，我花了好长时间才基本入门，看了一本cuda并行编程的书，又看了triton的官方文档，后来才逐渐学明白了，自己也动手写了一些kernel。说会遇到的挑战，我觉得遇到的最大的挑战就是关于转置算子，因为任务书有个指标就是转置算子的加速，虽然实现triton转置算子很简单，但是最后要和SAS对比的话必须得放在模型上跑（SAS是闭源的，不能直接调用底层算子，只能测出来整个一层的推理耗时），但是深度学习又没有什么直接运用转置的层，当时就困扰了很长时间，我感觉就是写任务书的人也不懂，随便写了个转置算子上去，最后麻烦的只有我一个人。后来我就索性用attention kernel替代转置算子了，跟导师讨论了他也觉得没问题，就这么干了。所以印象比较深的挑战就是这个吧。



### 说服别人的经历

SAS第三方测试：第一次跟第三方机构谈的时候，





### 委屈的经历/自己觉得做的好但被别人否定的经历



### 工作上什么时候会有压力，如何解决








## 面试记录

| Date                                 | 记录                                                         | 感受                                                         | 结果       |
| ------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ---------- |
| 5/19 20：00 字节AI Infra             | 问了triton相关细节，手撕代码没过                             | 第一次面试无经验，感觉被拷打                                 | 无后续     |
| 5/29 11：00 蚂蚁 导师推荐            | 电话面试，问了项目相关和研究生相关经历                       | 面试官有点condescending                                      | 性格测试挂 |
| 6/5 19：00 海康威视                  | 面试官像个大学老师，一直在问我到底为什么对大模型感兴趣       | 感觉和考研复试面试差不多                                     | 挂         |
| 6/6 14：00 要务科技 机械星球         | 纯聊天，聊简历                                               | 没什么印象                                                   |            |
| 6/9 9：50 上海刑事科学技术院         | 自我介绍+项目，问我有没有LLM应用和微调方面的经历             | 方向不太匹配                                                 |            |
| 6/9 19：00 美团北斗                  | 自我介绍+项目拷打，问的比较深，还让我现场写kernel，还问了个c++的结构体 | 感觉面试官挺专业，估计没有二面了，自身实力不够               |            |
| 6/10 博世多模态                      | 英文自我介绍+问了多模态相关问题                              | 岗位根本不匹配，啥也没问就结束了                             |            |
| 6/11 京东 后端开发岗 北京            | 自我介绍+现场写softmax算子+问几个推理框架+编译器问题全都不会 | 面试官很专业，我啥也不会                                     |            |
| 6/16 18:00 快手大模型训推优化        | 自我介绍+经历提问+MLA解释+layernorm kernel + 算法题          | 面试过程很轻松自由，写代码的时候面试官在干其他事             |            |
| 6/18 10：10 南京某国企 夏令营        | 自我介绍+微调和RAG区别+项目介绍                              | 没说完疯狂打断，面试体验很差，逆天面试官不尊重人，还问导师是不是教授，为什么不考南大 |            |
| 6/23 15：00 字节 搜索                | l2 cache怎么提高的命中率+triton底层+C++ struct内存占用+ 树后序遍历递归+非递归 | 问的问题非常细，                                             |            |
|                                      |                                                              |                                                              |            |
|                                      |                                                              |                                                              |            |
| 9/3 16:00 快Star-X AI Infra 推理引擎 |                                                              |                                                              |            |
|                                      |                                                              |                                                              |            |
|                                      |                                                              |                                                              |            |
|                                      |                                                              |                                                              |            |
|                                      |                                                              |                                                              |            |
|                                      |                                                              |                                                              |            |
|                                      |                                                              |                                                              |            |





## triton相关问题整理

### Triton 编译器架构与设计

- **Q1: Triton 编译器的整体架构如何？**
   **答：** Triton 采用基于 LLVM 的模块化编译架构。使用 `@triton.jit` 装饰 Python 函数时，编译器会对函数的 AST 进行解析，生成一种称为 Triton-IR 的中间表示（IR），其中多维数据块（tiles）是一级公民。Triton-IR 是一种未优化、机器无关的中间表示。之后，编译器对 Triton-IR 进行优化（如自动划分并行任务、优化内存访问、张量乘加融合等），生成 Triton-GPUIR（TTGIR），再通过 LLVM 后端生成最终的 PTX 代码。简而言之，Triton 将 Python kernel 降低为多层 IR，再经过 LLVM 生成高效的 GPU 汇编。

- **Q2: Triton-IR 有哪些特点？**
   **答：** Triton-IR 与传统 LLVM IR 不同，它以**块级（tile-level）**运算为基础，直接支持向量和矩阵运算。编译器可以基于 Triton-IR 自动完成许多优化任务：例如，识别出计算密集的块级操作（如 `tl.dot` 等），自动将数据缓存到共享内存，并通过活跃度分析决定何时同步。在并行度方面，Triton 能自动将不同 kernel 实例分配到不同 SM（工作组级并行），并在单个 SM 内根据迭代空间自动划分工作到各 warp/线程。另外，Triton-IR 和 Triton-GPUIR 都基于 MLIR（LLVM 项目的子项目），使得 Triton 能利用 LLVM 生态的优化算法。

- **Q3: Triton 如何支持 Kernel Fusion？**
   **答：** Triton 鼓励在单个 kernel 中融合多步运算，以减少内存读写。例如，在实现 Softmax 时，可以在一个 kernel 内依次完成求最大值、指数化、归一化等步骤，而不把中间结果写回显存。这样可以避免多次 DRAM 读写，大大提高带宽利用率。Triton 框架提供丰富的张量操作，用户直接在一个 kernel 中调用多种运算即可实现融合。相关示例表明：此类融合往往能带来几倍的加速（例如矩阵乘加 + ReLU 的融合相较两次分开运算可接近 2× 性能提升）。Triton 自身并不自动融合不同 kernel，融合需要在编写 kernel 时由开发者完成，但编译器可对融合后的代码进行优化。

- **Q4: Triton 有哪些自动调优 (autotuning) 机制？如何使用？**
   **答：** Triton 提供 `@triton.autotune` 装饰器让用户指定多组可能的配置，框架会在运行时测试这些配置并选出最优。用户传入一系列 `triton.Config`（指定不同的 BLOCK_SIZE、num_warps 等）和调优键（key），当相应参数变化时，所有配置都会被评测。例如，可以写：

  ```python
  @triton.autotune(configs=[
      triton.Config(kwargs={'BLOCK_SIZE': 128}, num_warps=4),
      triton.Config(kwargs={'BLOCK_SIZE': 1024}, num_warps=8),
    ], key=['N'])
  @triton.jit
  def kernel(..., N, BLOCK_SIZE: tl.constexpr):
      ...
  ```

  在首次运行时，Triton 会运行并比较各配置耗时，选出最快的。通过设置环境变量 `TRITON_PRINT_AUTOTUNING=1`，还可以让 Triton 在终端打印每个 kernel 自动调优的结果（包括耗时和最佳配置）。在实际工程中，可将调优结果缓存到文件或数据库（类似 XLA 的持久化缓存机制），以便在后续相同情形下直接加载加速启动。

- **Q5: Triton Kernel 的编写和调试方法有哪些？**
   **答：** 编写 Triton Kernel 就像写 Python 函数：在函数前加 `@triton.jit` 装饰，用 `triton.language` 提供的张量操作 (`tl.load`, `tl.store`, `tl.dot`, `tl.exp` 等) 来构造计算。调试方面，Triton 提供四个内置操作符：`static_print`/`static_assert`（用于编译时检查）和 `device_print`/`device_assert`（用于运行时检查，仅在设置 `TRITON_DEBUG=1` 时生效）。此外，还可以用 CPU 解释模式来调试：设置环境变量 `TRITON_INTERPRET=1`，使 Triton 在 CPU 上用 NumPy 逐步模拟执行 kernel，方便打印和单步调试。对于 GPU 上的内存访问错误，也可以使用 NVIDIA 的 compute-sanitizer 等工具进行动态检测。

### Triton 与 CUDA 的对比

- **Q6: Triton 相对 CUDA 的主要优点是什么？**
   **答：** Triton 最大的优势在于更高层次的抽象和易用性。开发者可以用 Python 编写 kernel，免去了 CUDA C++ 的冗长模板和错误陷阱。Triton 编译器会自动处理许多低级细节，比如自动对齐内存访问和共享内存管理，显著降低了编程复杂度。此外，Triton 在一个 kernel 中融合多操作（如 Softmax 的多步运算）也十分方便，能充分利用硬件带宽。
- **Q7: Triton 相对 CUDA 的缺点或局限有哪些？**
   **答：** Triton 目前主要针对 NVIDIA GPU 设计，不支持其他设备。与 CUDA 相比，Triton 在跨 SM 的调度上仍然需要用户手动指定 kernel 网格（grid）大小。如果需要非常精细地控制线程拓扑或使用最新硬件特性（例如直接使用新指令集、特殊缓存策略等），CUDA 提供的底层 API 会更灵活。CUDA 生态成熟，已有大量经过深度优化的库（如 cuBLAS、cuDNN、CUTLASS 等），某些标准算子使用这些库可能比自己写 Triton kernel 更方便、高效。此外，Triton 的 JIT 编译也意味着启动开销（首次运行时编译）会比静态 CUDA 稍大，对于极小的、频繁启动的 kernel 可能影响性能。
- **Q8: Triton 与 CUDA 在性能调优上的差异？**
   **答：** 在性能上，Triton 生成的 kernel 往往与手写 CUDA 相当，尤其在算子融合和宽度匹配场景下表现良好。调优思路上，Triton 用户仍需考虑线程块大小（BLOCK_SIZE）、warp 数量等对带宽和计算的影响，也需避免 warp 分歧等问题。这些与 CUDA 调优类似。不同的是，Triton 会自动合并内存访问和共享内存使用，开发者可以少操心这些低级优化，但仍可手动调 `num_stages`、`num_warps` 等参数。性能调优时，可以使用 NVIDIA 的 Nsight Compute 等工具分析 kernel 的瓶颈（如内存访问是否合并、L2 缓存命中率等），还可以利用 `TRITON_PRINT_AUTOTUNING` 输出的日志来指导块大小选择。

### Triton vs CUDA 关键对比

| 特性                      | CUDA (手动)                          | Triton (自动)                                           |
| ------------------------- | ------------------------------------ | ------------------------------------------------------- |
| **内存对齐 (Coalescing)** | 手动优化（需要保证访问模式）         | 自动合并访问                                            |
| **共享内存管理**          | 手动分配/同步                        | 编译器自动识别、缓冲数据                                |
| **SM 内部线程调度**       | 手动分配线程块给 SM、设定 block 大小 | 编译器自动并行化、在 SIMD 单元中分配任务                |
| **SM 间线程调度**         | 手动设定 grid 大小                   | 手动（Triton 不跨 SM 自动分配）                         |
| **编程模型**              | C/C++ (CUDA API)                     | Python API (`@triton.jit` 函数)                         |
| **调试工具**              | cuda-gdb、printf                     | 内置 `static_print/device_print`、`TRITON_INTERPRET` 等 |
| **现有库支持**            | 丰富（cuBLAS, cuDNN 等）             | 主要依赖自定义 kernel 开发，库支持尚少                  |



### GPU 并行计算基础

- **Q9: CUDA 的线程模型：warp、thread、block 有何区别？**
   **答：** 在 CUDA 中，每个多处理器（SM）同时调度执行一个 warp 的指令，当前架构下一个 warp 包含 32 个线程。线程（thread）是最小执行单元，多个线程组成线程块（block），多个线程块组成网格（grid）。程序中通过配置每个 kernel 的 block 大小（每个 block 包含多少线程）和 grid 大小来控制并行度。一个线程块内的线程共享同一块 SRAM（共享内存），线程块之间可以看作无直接同步（除非使用全局同步原语）。概括：每个 block 有若干 warp，warp 内的 32 个线程同时执行同一条指令；多个 block 在多个 SM 上并发执行。
- **Q10: GPU 的内存层次结构有哪些？**
   **答：** CUDA 的内存层次从近到远依次是：寄存器 → 共享内存/L1 缓存 → L2 缓存 → 全局内存（显存）→ 主机内存。每个线程有私有的寄存器和 local 内存，每个线程块有共享内存供所有线程快速访问。全局内存对所有线程可见，但访问延迟最高。常量内存和纹理内存是只读缓存，优化特定的访问模式。此外，现代 NVIDIA GPU 有各层高速缓存（如 Ampere 的 L2 缓存），用以提高全局访问效率。调优时，应尽量多用共享内存和 L1 缓存来复用数据，减少对全局内存的直接访问。
- **Q11: 什么是内存合并 (coalescing)？如何避免冲突？**
   **答：** 内存合并指的是当同一个 warp 的线程同时访问相邻地址时，硬件会将多个内存访问合并为少量传输。例如在 Volta 及以后架构上，一个 warp 可以以 128 字节（32×4-byte）的块为单位访问，只要每个线程访问连续地址且对齐，就能打包为一次传输。反之，如果访问不对齐或散乱，就可能产生多个事务，降低带宽利用率。在 Triton 中，使用 `tl.arange` 等生成连续地址序列可以自动获得合并访问。另外，共享内存中也要避免**bank 冲突**，即同一时刻过多线程访问同一个 bank（16/32 位交错的片段）；编写代码时可通过添加填充或调整访问模式来消除冲突。
- **Q12: 什么是占用率 (Occupancy)？为什么重要？**
   **答：** 占用率定义为每个 SM 上**活跃 warp 数**与该 SM 最大活跃 warp 数的比值。换言之，若 SM 最多可并行 64 个 warp，当活跃的 warp 只有 32 个时，占用率为 50%。高占用率意味着有更多并发线程可执行，可以更好地隐藏指令和内存的延迟；但过高占用率也可能导致每线程资源减少（如寄存器变少，产生 spill），从而降低性能。通常用 CUDA 提供的 Occupancy API（如 `cudaOccupancyMaxActiveBlocksPerMultiprocessor`）或 Nsight 工具来评估占用率。在调优时，若发现占用率过低，可增大 block 大小或减少每线程使用的资源；若占用率已满，还需关注是否有其他瓶颈（如内存带宽）。


### 实际项目经验与性能分析

- **Q13: 如何在训练/推理流水线中集成自定义 Triton Kernel？**
   **答：** 在 PyTorch 中，可以直接在前向/后向函数里调用 Triton kernel，就像调用普通的 tensor 操作。例如，定义 Triton kernel 后，在训练循环中用 `add_kernel[grid](args)` 这样启动。对于 PyTorch 2.x，还可通过 `torch.compile` 将 Triton kernel 与模型编译到一起。PyTorch 官方教程展示了如何在 `torch.compile` 环境下嵌入 Triton 向量加法 kernel，实现了将 Triton 优化计算加入到模型中的流程。在更高层面，可以将 Triton kernel 封装为自定义算子供前端调用，或借助 TorchDynamo/Inductor 等工具自动识别并转换。总体思路是：确保输入输出是 CUDA tensor，然后像使用其他 CUDA 运算那样调用 Triton kernel 即可无缝集成。
- **Q14: 如何对 Triton Kernel 进行性能分析（Profiling）？**
   **答：** Triton kernel 其实是 CUDA kernel，所以可以使用 NVIDIA 提供的各种工具进行分析。常用的方法包括：使用 `nvprof`（旧版已弃用）、NVIDIA Nsight Compute、Nsight Systems 等采集 kernel 级别的指标。这些工具可以测量每个 kernel 的执行时间、内存吞吐、warp 效率、指标带宽等。使用 Triton 时，也可以用 `TRITON_PRINT_PERF_COUNTERS` 环境变量让 Triton 在运行时输出一些统计信息。针对 PyTorch 模型级的瓶颈，则可用 PyTorch Profiler（`torch.profiler`）结合 TensorBoard 可视化来定位是哪个操作耗时最多。分析时应关注是否有内存合并问题、寄存器溢出、分支分歧等因素。
- **Q15: 进行性能分析时应关注哪些指标？**
   **答：** 常见关键指标包括：**内存带宽利用率**（看全局/共享内存访问是否为瓶颈）、**SM 利用率**（活跃 warp 数，反映占用率）、**warp 效率**（包括分支分歧情况）、**寄存器和共享内存占用**（高占用可能导致并行度下降）、以及 **Tensor 核心利用率**（若算子可用 Tensor Core 应检查是否被充分利用）。例如，要检查全局内存访问是否合并，可看“global load throughput”是否接近理论带宽；若性能低，可检查合并率（nsight 中常有“global_transactions_per_request”指标）。其他如延迟（时序）、L2 缓存命中率等也值得关注。综合这些指标，判断是计算瓶颈还是内存瓶颈，据此修改 kernel 参数或算法。





## 八股笔记

### 数据并行DP/流水线并行PP/张量并行TP

| DP                  | PP                                                  | TP                      |
| ------------------- | --------------------------------------------------- | ----------------------- |
| 切分数据            | 切分模型                                            | 切分矩阵                |
| 每个GPU保存完整模型 | 每个GPU只有模型的一部分                             | 就是cuda/triton并行计算 |
|                     | 进一步划分micro batch,按流水线方式分别给每个GPU处理 |                         |

DP+PP:

+ 宏观数据并行
+ 微观流水线并行

ZeRO DP:

+ ZeRO: Zero Redundancy Optimizer
+ 用于解决DP的缺点：浪费显存（每个GPU都存放模型+梯度+optimizer state）
+ 方法：切分整个模型的参数，每个GPU只存放一部分参数，计算时需要所有GPU先通信，得到完整模型再计算
+ **切分逻辑和流水线并行不同：**
  + ZeRO DP是横向切：切的是每一层的参数，计算时需要GPU通信
  + PP是纵向切，切的是模型的层，不需要GPU通信





### Optimizer

#### SGD

SGD（随机梯度下降）和原始的梯度下降的主要区别是：**每次更新参数时用的数据量不同**。

+ SGD每次用一个batch的梯度更新网络
+ 而原始GD必须把整个数据集梯度全部算好后再更新网络



#### momentum

普通梯度下降：
```
参数 = 参数 - 学习率 × 当前梯度
```

加了动量的梯度下降

```
速度 = 衰减率 × 之前的速度 - 学习率 × 当前梯度
参数 = 参数 + 速度
```



#### SwigLU





### vLLM

#### PagedAttention

> KV Cache 被划分为块。块在内存空间中不需要连续。

​	由于块blocks在内存中不需要连续，因此我们可以像在操作系统的虚拟内存中一样以更灵活的方式管理键和值keys & values：可以将块视为页面pages，将tokens视为字节bytes，将序列sequences视为进程processes。序列的连续 ***逻辑块*** ***logical blocks*** 通过 **块表** **block table** 映射到非连续 **物理块** ***physical blocks***。物理块在生成新tokens时按需分配。







### Temperature/top-P/top-K

![image-20250824143904935](./../img/typora-user-images/image-20250824143904935.png)

**temperature**通过对logits的值进行缩放（除以T)，从而修改softmax后的概率分布

 <img src="./../img/typora-user-images/image-20250824144126434.png" alt="image-20250824144126434" style="zoom: 50%;" /> <img src="./../img/typora-user-images/image-20250824144148185.png" alt="image-20250824144148185" style="zoom:50%;" />

+ T取值一般在0.0~2.0, 最常用范围是0.2~1.0
+ $T\in(0.2, 0.6)$: 低温度，logits放缩后绝对值会变大，softmax后又会放大logits值之间的差距，
  从而**高概率token的概率会变得更高，低概率token的概率会变得更低**，概率分布更集中->模型输出更准确、减少随机性
+ $T\in(0.7, 1.0)$: 中等温度，logits放缩后绝对值会变小，softmax后会压缩logits值之间的差距，
  从而**高概率token的概率会降低，低概率token的概率会提高**，概率分布更平坦->增加模型创造性**（GPT默认0.7）**
+ $T > 1$: 高温度，容易产生幻觉。适用于艺术性生成场景。



**top-P/top-K**用与选择输出token的范围, 概率太低的直接不考虑。（**不是每次都在整个vocab size中选token输出！）**

![image-20250824145210948](./../img/typora-user-images/image-20250824145210948.png)

+ top-K指定token数量: 直接选择概率最高的K的token
+ top-P指定token总和: 选择概率总和>=P的最少token（即从高到低开始选，总和达到P时停止）

**Top-P** 通常比 **Top-K** 更受欢迎，因为它能根据概率分布的实际情况动态调整考虑的token数量，使其更灵活。而 Top-K 总是考虑固定数量的选项，这可能在概率急剧下降时过于宽泛，或者在概率分布分散时过于狭窄



### RAG vs Fine-tuning

我感觉这俩好像不是一个维度的...

SFT就是拿另一个比如说某个领域的数据集，把这个模型继续训练一下，那训练的时候肯定会修改模型参数嘛，而且比较耗资源。

RAG是在跟模型对话的时候起作用的，就是他会去查一个外部的知识库，然后把查到的内容当成prompt跟你的输入一起丢给大模型，最后得到输出。那这个肯定不会修改模型参数的。

而且RAG和SFT是可以并存的呀，微调之后还是可以RAG查外部知识库的。所以RAG优点就是可以获取最新的知识，也不需要训练模型。SFT的话就会把模型整个能力、风格都改掉，一般都是企业要搞一个领域大模型才会话很多资源去微调。





### MLA & RoPE

> #### MHA & MQA & GQA
>
> ![image-20250824153702982](./../img/typora-user-images/image-20250824153702982.png)





## 面经整理

*Thinking...*

> 这是一系列关于大模型推理加速的技术面试问题，需要用口语化的方式回答，体现专业性但不要太正式，可以加一些语气词模拟面试中的思考过程。我需要逐一回答这些问题。
>
> 1. batch_size 可以无限增加吗？
> 2. 大模型推理性能指标、影响因素、私有化部署方案
> 3. 推理框架适配新模型和硬件
> 4. LLM推理流程
> 5. AWQ量化原理和过程
> 6. Paged Attention原理和实现
> 7. 性能瓶颈分析
> 8. vLLM PagedAttention性能提升原因
> 9. 异步编程和LLM推理异步实现
> 10. 分布式推理方法
> 11. MOE并行加速
> 12. Triton编译过程
> 13. Llama模型forward推理输入
> 14. LLM推理两阶段原因
> 15. per channel和per group量化区别
> 16. PyTorch显存管理机制
>
> 我需要用口语化的方式，加一些"嗯"、"那个"、"其实"等语气词，模拟真实面试中的表达。

好的，我来一个个回答这些问题：

**1. batch_size 可以无限增加吗？**

嗯，这个不能无限增加的。主要有几个限制因素：首先是显存限制，batch_size增大会导致中间激活值、KV cache等显存占用线性增长，很容易就OOM了。其次是延迟问题，虽然吞吐量会提升，但是单个请求的延迟也会增加，因为要等整个batch处理完。还有就是实际场景中，请求往往是不规则到达的，强行凑大batch可能会让用户等待时间过长。

**2. 大模型推理性能指标有哪些、影响性能的因素有哪些？私有化部署方案？**

性能指标主要看三个：吞吐量（tokens/s或者requests/s）、延迟（首token延迟TTFT和总延迟）、还有资源利用率。

影响因素嗯...挺多的，硬件层面有GPU算力、显存带宽、PCIe带宽这些；软件层面有模型并行策略、KV cache管理、算子融合优化、量化等；还有就是batch调度策略、内存管理这些。

私有化部署的话，通常会用容器化部署，比如Docker+Kubernetes，配合GPU调度器。推理框架选择vLLM、TensorRT-LLM或者TGI这些，根据具体需求做trade-off。还要考虑负载均衡、监控告警、自动扩缩容这些运维需求。

**3. 给定一个指定推理框架如 vllm/lightllm 如何适配新的模型和硬件？**

适配新模型的话，首先要在模型配置文件里添加新模型的架构定义，然后实现对应的forward函数，主要是attention、MLP这些核心组件。还要处理tokenizer、模型权重加载这些。vLLM的话就是在model registry里注册新模型。

硬件适配主要是算子层面的优化，比如针对新GPU写CUDA kernel，或者用Triton写算子。还要考虑显存管理、通信库适配这些。其实现在很多框架都有plugin机制，相对比较友好。

**4. llm 的推理流程**

LLM推理分两个阶段：prefill和decode。

Prefill阶段是把整个prompt一次性输入，并行计算所有position的attention，生成完整的KV cache，然后预测第一个token。这个阶段是compute-bound的，GPU利用率比较高。

Decode阶段是autoregressive生成，每次只生成一个token，然后append到sequence里继续生成下一个。这个阶段是memory-bound的，因为每次都要访问之前的KV cache，但计算量相对较小。

**5. AWQ 量化原理、过程详细介绍**

AWQ的核心思想是不是所有权重都同等重要，激活值大的权重对模型性能影响更大。

具体过程是这样的：首先用少量校准数据统计每个channel的激活值分布，找出activation magnitude比较大的channel，这些被认为是"重要"的权重。然后对重要的权重保持较高精度，对不重要的权重进行更激进的量化。

实现上，AWQ会对权重做一个scale变换，让重要的权重scale变大，不重要的变小，然后再做uniform quantization。这样就实现了adaptive的效果。相比GPTQ，AWQ的优势是推理时不需要dequantization，速度更快。

**6. 讲一讲Paged Attention的原理和实现**

嗯，这个设计很巧妙。传统的做法是预先分配max_seq_len大小的连续KV cache，但这样会有很多浪费，因为序列长度通常差异很大。

Paged Attention的思路是把KV cache分成固定大小的page，比如每个page存16个token的KV。这样就可以按需分配，用多少分配多少。具体实现上，维护一个page table记录每个sequence的KV cache分布在哪些page上，attention计算时根据page table找到对应的KV值。

好处是显存利用率高，支持动态batching，还能实现sequence之间的page共享（比如prefix相同的情况）。

**7. 性能瓶颈如何分析？**

分析性能瓶颈我一般是分层来看的。首先用profiler看整体的timeline，比如nsight或者torch profiler，看是compute-bound还是memory-bound。

如果是compute-bound，重点看kernel的occupancy、SM利用率这些；如果是memory-bound，就看memory throughput、cache hit rate。

然后细化到具体算子，看哪个算子耗时最多，是否有优化空间。还要看GPU之间的通信开销，P2P带宽利用率这些。最后是框架层面的，比如调度开销、内存分配开销等。

**8. 为什么 vllm 的 PagedAttention 相比之前的一次性分配固定长度 max_seq_len 的 kv cache 空间能提升性能？**

主要是两个方面的提升：

显存利用率大幅提升。传统方法要按最大长度分配，但实际序列长度差异很大，导致大量浪费。PagedAttention按需分配，利用率能从20-40%提升到90%以上。

支持更灵活的batching策略。因为不用预先知道序列长度，可以动态添加删除序列，实现continuous batching，这样GPU利用率更高，延迟也更低。

**9. 什么是异步，如何用 python 实现异步功能？llm 推理的异步如何实现？**

异步就是不阻塞等待的编程模式，可以在等待I/O时去处理其他任务。

Python实现异步主要用asyncio，定义async函数，用await等待异步操作。比如：

```python
async def generate():
    result = await model.generate_async()
    return result
```

LLM推理的异步主要体现在：请求排队和batch组装是异步的，不会阻塞新请求的接收；推理过程可以pipeline，prefill和decode可以交叉进行；多个请求的生成过程是并发的。

具体实现上，通常有一个异步的request queue，一个background的inference engine，通过callback或者future来返回结果。

**10. 大模型推理框架的分布式推理有哪些方法？**

主要有几种并行策略：

Tensor Parallel（TP）：把模型按层内切分，比如attention的多头、MLP的参数都分布到不同GPU上，需要频繁的all-reduce通信。

Pipeline Parallel（PP）：按层间切分，不同GPU负责不同的layer，通信相对较少，但可能有bubble。

Data Parallel：每个GPU都有完整模型，处理不同的batch，适合小模型。

还有Sequence Parallel，把sequence dimension也做切分。实际部署时通常是多种策略组合，比如TP+PP。

**11. MOE 怎么实现并行加速？**

MOE的并行主要是Expert Parallel，把不同的expert分布到不同的GPU上。

具体实现是这样的：首先router决定每个token要路由到哪些expert，然后做一个all-to-all通信，把token发送到对应的expert所在的GPU。expert计算完后，再做一次all-to-all把结果发回去。

关键是要做load balancing，避免某些expert负载过重。还有就是通信优化，可以把多个小的all-to-all合并，减少通信次数。

**12. 介绍一下triton编译过程**

Triton的编译过程是这样的：首先Python代码会被parse成Triton IR，这是一个高层的中间表示。然后做一系列优化，比如循环优化、内存访问优化。

接下来转换成LLVM IR，这时候会做更多的low-level优化。最后生成PTX代码，再编译成CUDA binary。

整个过程中，Triton会自动处理很多细节，比如block tiling、memory coalescing、bank conflict避免这些，让用户专注于算法逻辑。

**13. llama 模型 forward 推理的输入是什么？**

主要输入有几个：input_ids（token序列）、position_ids（位置编码）、attention_mask（指示哪些位置需要attend）。

对于端侧推理框架，通常还会有kv_cache作为输入和输出，用于存储之前的key-value。casual mask一般是在attention计算时动态生成的，确保只能attend到之前的位置。

还可能有一些可选的输入，比如past_key_values、use_cache这些控制参数。

**14. LLM 推理为什么需要分成两个阶段？不分可以吗，为什么？**

这个...其实是由autoregressive生成的特性决定的。

Prefill阶段可以并行计算所有position的attention，因为prompt的所有token都已知，可以一次性处理，这时候是compute-bound的，GPU利用率高。

但是decode阶段必须sequential，因为下一个token依赖于前面所有token的结果，没法并行。而且这个阶段是memory-bound的，主要开销在访问KV cache。

如果不分阶段，那就没法充分利用prefill阶段的并行性，整体效率会很低。所以这个分阶段是必须的，针对不同阶段的特点做专门优化。

**15. per channel 和 per group 量化有什么区别，为什么不用 per channel 量化？**

Per channel是每个output channel用一个scale，粒度更细，量化精度更高。Per group是把多个channel分成一组，一组用一个scale。

虽然per channel精度更高，但是dequantization的时候计算开销更大，因为要对每个channel做不同的scale操作。Per group的话，可以用SIMD指令batch处理，速度更快。

而且现在的量化算法，比如AWQ、GPTQ，在per group的粒度下已经能取得很好的精度，所以工程上更倾向于用per group，在精度和速度之间找平衡。

**16. pytorch 的显存管理机制了解吗？**

PyTorch的显存管理有几个特点：首先是lazy allocation，只有真正需要时才分配显存。然后是memory pool机制，释放的显存不会立即还给系统，而是放在pool里复用，避免频繁的malloc/free。

还有就是引用计数机制，tensor不再被引用时会被释放。但是有时候会有显存碎片问题，这时候可能需要手动调用empty_cache()。

另外，PyTorch还支持显存映射、unified memory这些特性，可以在CPU和GPU之间共享内存。现在还有一些实验性的功能，比如memory snapshot、显存预分配这些。
