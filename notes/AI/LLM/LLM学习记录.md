### GPT 术语

1. lm_head:
   softmax前的最后一个线性层, 调整输出大小为token种类数，经过softmax后每个节点就是所代表token的概率
2. logits:
   即经过lm_head得到的输出（softmax前），传递到softmax函数后，得到的输出就是每个类别的概率分布。
   logits这个词来源于“log-odds”，在统计学中，它表示事件发生的对数几率。虽然这个词不一定直接与logits的计算相关，但在机器学习中，logits通常用于表示分类任务的输出。



### 关于Positional Encoding

##### 1. 推理时，任何输入的pos都是一样的，那还怎么区分不同输入的区别？

> 在推理时，虽然所有输入的 `pos`（位置编码）序列是一样的（即从 `[0, T)` 的整数序列），但输入序列的**token嵌入**（`token embeddings`）是不同的。这就是区分不同输入的关键所在。
>
> 1. **位置编码的作用**
>
> 位置编码 (`pos_emb`) 的作用是为每个位置提供唯一的位置信息。这是因为 Transformer 模型本身是**无序的**，无法自然区分序列中的位置信息（不像 RNN 这样的模型有序处理数据）。因此，必须通过显式的位置编码来给出序列中的位置信息。
>
> 在推理时，每个位置的 `pos_emb` 确实是相同的，但这是为了告诉模型：**这是序列中的第几个位置**。换句话说，位置编码仅仅起到了让模型知道第 `i` 个位置的作用。
>
> 2. **token嵌入的作用**
>
> 不同输入的 `token embeddings` 是不同的，即 `tok_emb` 由输入的具体 tokens 决定：
>
> ```python
> tok_emb = self.transformer.wte(idx) # token embeddings of shape (B, T, n_embd)
> ```
>
> - `tok_emb` 是由输入的 token 序列 `idx` 通过词嵌入（`wte`）得到的。
> - 每个输入序列的 tokens 是不同的，因此其对应的 token embeddings 也不同。
> - 这些 token embeddings 代表了每个 token 的语义信息，和位置编码无关。
>
> 3. **结合位置和 token 嵌入**
>
> 最终，模型将 token 嵌入和位置嵌入相加：
>
> ```python
> x = tok_emb + pos_emb
> ```
>
> 这个加法操作的结果是，**模型既能识别 token 的内容（通过 `tok_emb`），又能知道该 token 在序列中的位置（通过 `pos_emb`）**。即使每个输入的 `pos` 是一样的，不同的 token 嵌入会让模型区别开不同的输入序列。
>
> 总结
>
> - **`token embeddings`** 提供了每个输入序列中 token 的语义信息，不同的输入有不同的 token 嵌入。
> - **`position embeddings`** 提供了 token 在序列中的位置信息，帮助模型在无序的情况下理解序列的顺序。
> - 在推理时，不同输入的区别主要通过 `token embeddings` 来表示，而 `position embeddings` 则帮助模型理解 token 的相对位置。



##### 2. 既然pos_emb只负责让模型知道每个token在模型中是第几个位置，那直接用整数序列作为pos_emb不就好了吗？为什么要用embedding？甚至还要把embedding table作为可训练的参数？

> 使用位置嵌入（`position embeddings`）而非直接使用整数序列的原因在于，**位置嵌入可以捕捉到更复杂的位置相关性，而不仅仅是单一的线性顺序**。具体来说，有以下几个原因：
>
> 1. **线性整数序列无法表达复杂的位置信息**
>
> 如果直接使用整数序列作为位置编码（例如位置0对应0，位置1对应1），模型只会简单地知道每个 token 的顺序位置，但无法学到更多复杂的模式或关系。
>
> 例如，模型可能需要知道：
> - 两个 token 的相对位置是否有规律性（如间隔一定距离的词）。
> - 远距离 token 之间的依赖关系。
>   
>
> 使用整数序列可能过于简单，难以有效捕捉这种复杂的模式，而嵌入的方式可以为模型提供更丰富的表达能力。
>
> 2. **嵌入提供了更灵活的表示**
>
> 嵌入将位置编码映射到高维空间，使得每个位置不仅仅是一个整数，而是一个**高维向量**。这个向量可以通过训练学习到更多关于每个位置与序列中的其他位置的复杂关系。这种灵活性帮助模型从数据中自动学到：
> - 不同位置的 token 如何相互作用。
> - 某些固定位置对上下文理解的重要性（例如句首或句尾可能有特定意义）。
>   
>
> 通过可训练的嵌入，模型可以自行调整每个位置的表示，而不是受限于固定的整数序列。
>
> 3. **嵌入的可训练性提升模型性能**
>
> 将位置嵌入设为可训练参数意味着模型可以在训练过程中自适应地优化这些位置嵌入。这使得模型可以：
> - 自动找到适合当前任务的位置信息表示。
> - 学习不同位置对任务的不同重要性。
>
> 通过训练，模型能够学会最适合特定任务的位置信息表示，提升性能。这比直接使用固定的整数序列要灵活得多。
>
> 4. **高维空间中的位置关系更加复杂**
>
> 高维空间中的位置编码提供了更复杂的关系表示。在 1D 空间中，整数序列仅代表简单的线性关系，而高维嵌入可以捕捉到：
> - 非线性的相对位置信息（例如，位置2和位置10的关系可能比位置2和位置3更重要）。
> - 通过多维度编码，表示不同位置的多个方面的关系。
>
> 这种复杂的位置信息有助于模型理解长距离依赖和复杂上下文关系。
>
> 5. **经验效果**
>
> 实践中，已经有许多实验表明，使用可训练的位置嵌入比使用简单的整数序列或静态编码（如正弦/余弦位置编码）能够更好地提升模型性能。因此，主流模型（如 GPT、BERT）都选择了使用可训练的嵌入表来表示位置。
>
> 总结
>
> 虽然直接用整数序列作为位置编码是可行的，但这种方法过于简单，限制了模型学习复杂位置关系的能力。使用可训练的嵌入表作为位置编码能够让模型学习更丰富的位置信息，并且在实践中被证明能够提升模型的性能和泛化能力。



##### 3. 不同的输入，其中蕴含的复杂位置信息不应该都是不同的吗，那为什么pos_emb和输入的内容无关呢？

> 这是一个非常好的问题。确实，**不同的输入之间蕴含的位置信息可能是不同的**，但位置编码（`pos_emb`）和输入内容无关的设计有其合理性，主要原因在于：
>
> 1. **位置编码只提供序列中的相对位置信息**
>
> 位置嵌入的作用是**让模型知道每个 token 处在序列中的哪个位置**，但并不需要直接考虑 token 的具体内容。**位置本身是序列的固有属性**，与内容无关。换句话说，位置编码的目的是帮助模型理解序列中的**结构信息**，比如句子中的第一个词和第二个词之间的相对顺序，而不需要知道这些词具体是什么。
>
> 在自注意力机制中，模型通过结合位置编码和 token 编码来学习输入之间的关系。通过使用相同的位置信息，模型可以在相同位置上学习不同输入的不同模式，而不需要每次根据输入内容动态生成位置编码。
>
> 2. **分离 token 和位置信息使模型更具通用性**
>
> 通过将位置信息和内容信息分离，模型可以更灵活地处理不同的输入。这种设计的好处在于，**无论输入的具体 token 是什么，位置信息是相同的**。这种分离有以下优势：
> - **减少复杂性**：模型不需要同时考虑每个 token 的内容和它在序列中的位置，简化了模型的学习过程。
> - **泛化性强**：模型可以将同样的位置嵌入应用于不同的输入，这有助于模型在处理不同输入时，仍然能够识别和利用位置信息来捕捉序列结构。
>   
>
> 例如，两个不同的句子 `"The cat sat on the mat"` 和 `"The dog lay on the bed"` 的 token 编码会不同，但它们的位置信息是一样的。这样，模型可以通过位置编码了解它们的结构，而 token 编码提供了具体的词语信息。
>
> 3. **自注意力机制能学习内容和位置的交互**
>
> 虽然位置编码和输入内容是独立的，但在**自注意力机制**中，它们的交互通过模型的层次处理逐渐被捕捉。具体来说，**自注意力机制通过加权输入序列中的每个位置**，使模型能够根据上下文调整对不同位置的关注程度。位置编码与内容无关，但通过与 token 编码的结合，自注意力机制可以捕捉到不同 token 在不同位置上对最终结果的贡献。
>
> 因此，虽然位置编码本身与输入内容无关，**模型可以通过自注意力机制动态调整不同位置的权重**，从而间接捕捉位置信息和内容之间的复杂交互。
>
> 4. **位置信息是相对固定的，内容信息是动态的**
>
> 在自然语言处理中，序列的位置信息通常是相对固定的，**相同长度的序列在同一个位置上总是对应相似的相对位置**。例如，句子的开头、中间、结尾通常有不同的重要性。
>
> 而**内容信息是动态的**，即不同的输入有不同的 token 表示。通过将位置信息编码为可训练的嵌入向量，模型可以自动学习在特定任务中哪些位置更加重要，并根据不同内容灵活处理序列中的位置信息。位置信息不变使模型更容易泛化到不同的输入内容，而无需为每个输入重新生成位置信息。
>
> 5. **简化计算复杂度**
>
> 如果位置编码依赖于每个输入内容，那么生成每个 token 的位置信息将更加复杂，导致训练和推理的计算复杂度增加。**将位置编码设为与内容无关的固定嵌入，不仅计算简单，还保持了足够的灵活性**，可以让模型处理各种复杂任务。
>
> 总结
>
> 位置编码和输入内容的独立性是有意的设计。位置嵌入的目标是捕捉**序列中的相对位置信息**，而模型通过自注意力机制和 token 编码来学习**内容和位置信息的交互**。这种设计既简化了计算复杂度，又增强了模型的灵活性和泛化能力。



##### 4. "The cat sat on the mat"和"I don't know what you mean"位置信息也是一样的，难道他们的结构相似吗？

> 虽然 `"The cat sat on the mat"` 和 `"I don't know what you mean"` 这两个句子的位置信息是一样的（即它们的 token 位于序列中的相对位置相同），但它们的结构并不相似。从位置编码的角度来看，**位置编码只是提供了序列中每个 token 的位置**，而模型并没有直接依靠位置信息来理解句子的语法结构或语义关系。
>
> 1. **位置信息是相对的，而不是句法结构的反映**
>
> 位置编码仅表示每个 token 在句子中的序列位置，它不会告诉模型词汇之间的语法关系或句子的语义结构。因此，虽然两个句子的位置编码相同，但这只是因为它们的 token 处在相同的相对位置上。**位置编码本身并不反映句子的语法或语义结构**，这些信息是通过内容（token 编码）和模型中的自注意力机制学习到的。
>
> 比如在 `"The cat sat on the mat"` 里，`"The"` 作为第一个 token，它在句子中充当定语；而在 `"I don't know what you mean"` 中，`"I"` 作为第一个 token 是主语。虽然它们在序列中的位置是相同的，但它们的语法角色和意义完全不同。这部分差异是通过 token 编码和模型内部的学习机制捕捉到的。
>
> 2. **自注意力机制捕捉到复杂的内容-位置交互**
>
> 虽然位置编码本身并不区分句子结构，但模型的自注意力机制可以通过结合 token 编码和位置信息，学习到输入内容的句法和语义结构。模型会根据每个 token 在不同位置的含义、上下文中的作用等，调整对不同位置的注意力权重。这就是为什么即使不同句子使用相同的位置信息，模型仍然能够识别出它们的不同结构和语义。
>
> 例如，模型可以学习到在 `"The cat sat on the mat"` 中，`"cat"` 和 `"`sat`" 之间有谓语关系，而在 `"I don't know what you mean"` 中，`"I"` 和 `"`don't"` 之间有否定关系。这些信息并不是由位置信息直接提供的，而是通过自注意力机制从 token 之间的交互中学习的。
>
> 3. **位置编码并不描述句子的复杂语法结构**
>
> 位置编码的设计目标是帮助模型**理解 token 在序列中的相对位置**，而不是提供句法或语义结构信息。这意味着它只用于确保模型可以区分输入序列中的不同位置，比如区分第一个词和第二个词的位置。至于句子的复杂结构，是模型通过大量数据训练后自动学习的。
>
> 因此，尽管这两个句子的位置信息相同，**模型通过 token 编码和自注意力机制能够学习到它们完全不同的句法和语义结构**。
>
> **总结**
>
> 位置编码仅仅告诉模型每个 token 在序列中的位置，而不提供语法或语义结构信息。模型通过自注意力机制结合 token 的编码，逐步捕捉句子的结构和语义。因此，即使两个句子的位置信息相同，模型也能够区分它们的不同结构和含义。





### KV cache

KV Cache 的核心是缓存之前计算的键（keys）和值（values），避免在生成新 token 时重新计算它们。对于位置 s ≤ t 的 token，其键值对 k(s), v(s) 由其嵌入计算得出，一旦嵌入固定，键值对也固定。当生成第 t+1 个 token 时：

+ 新 token 的嵌入是新计算的，产生新的 k(t+1), v(t+1)。
+ 之前 token 的 k(s) ≤ t, v(s) ≤ t 保持不变，因为它们的嵌入未变。

> 以下是根据你的要求，将数学公式部分修改为使用 `$`（行内公式）和 `$$`（块公式）的完整回答，保证在 Typora 中可以正确显示。我会直接提供修改后的内容，确保回答自洽且清晰。
>
> ---
>
> 你的疑问在于：从数值上看，在生成过程中，$k_1, v_1, k_2, v_2$ 的值与之前基于序列 `[token1, token2]` 计算的结果相同，但你认为每次调用 `model(x)` 时，`c_attn` 的权重（即用于生成 $q, k, v$ 的矩阵）应该会变化，导致每次计算的 $q, k, v$ 都不一样。我将通过详细的解释和具体例子，结合代码，说明为什么你的理解有误，以及为什么 $k_1, v_1, k_2, v_2$ 的值在生成过程中保持不变。
>
> ---
>
> ### 1. 模型的参数是固定的
>
> 在训练好的模型中，所有的权重参数，包括 `c_attn` 的权重，在推理（生成）阶段是固定的，不会随着输入序列的变化而改变。换句话说，`c_attn` 的权重矩阵 $W_q, W_k, W_v$ 在整个生成过程中保持不变。
>
> - **代码中的体现**：
>   - 在 `CausalSelfAttention` 类中，`self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)` 定义了一个固定的线性层，其权重在模型加载或训练完成后就固定了。
>   - 在生成过程中，每次调用 `model(x)` 时，`self.c_attn` 的权重保持不变，仅根据当前的输入 $x$ 进行前向计算。
>
> 因此，你的理解中“每次调用 `model(x)` 时，`c_attn` 的权重应该不一样”是错误的。在推理阶段，权重是固定的，不会发生变化。
>
> ---
>
> ### 2. 计算 $q, k, v$ 的过程
>
> 每次调用 `model(x)` 时，`self.c_attn(x)` 会对当前的输入 $x$ 进行线性变换，生成 $qkv$。具体来说：
> - 对于输入 $x = [x_1, x_2, \dots, x_T]$（形状为 $(B, T, C)$），`self.c_attn(x)` 生成 $qkv = [qkv_1, qkv_2, \dots, qkv_T]$，形状为 $(B, T, 3 \cdot C)$。
> - 然后拆分为：
>   - $q = [q_1, q_2, \dots, q_T]$，
>   - $k = [k_1, k_2, \dots, k_T]$，
>   - $v = [v_1, v_2, \dots, v_T]$。
>
> **关键点**：
> - $k_1$ 只依赖于 $x_1$ 和 $W_k$，
> - $v_1$ 只依赖于 $x_1$ 和 $W_v$，
> - 同理，$k_2$ 只依赖于 $x_2$ 和 $W_k$，$v_2$ 只依赖于 $x_2$ 和 $W_v$。
>
> 数学上：
> $$
> k_i = W_k \cdot x_i
> $$
> $$
> v_i = W_v \cdot x_i
> $$
> 其中，$W_k$ 和 $W_v$ 是 `self.c_attn` 线性层中用于生成键和值的权重矩阵，且这些权重在生成过程中是固定的。
>
> ---
>
> ### 3. 具体例子
>
> 让我们通过一个简化的例子来演示这个过程。假设：
> - 嵌入维度 $C = 2$，
> - 批次大小 $B = 1$（为了简化），
> - 序列长度 $T$ 随生成过程变化。
>
> #### **初始序列**：`[token1, token2]`，即 $T=2$
> - 假设 token1 的嵌入 $x_1 = [1.0, 0.0]$，
> - token2 的嵌入 $x_2 = [0.0, 1.0]$。
> - 输入 $x = [[1.0, 0.0], [0.0, 1.0]]$（形状为 $(1, 2, 2)$）。
>
> #### **线性层权重**（假设）
> 为了简化，假设 `self.c_attn` 的权重矩阵为：
> $$
> W = \begin{bmatrix}
> 1 & 0 & | & 2 & 0 & | & 3 & 0 \\
> 0 & 1 & | & 0 & 2 & | & 0 & 3 \\
> \end{bmatrix}
> $$
> 这里，`|` 分隔了 $W_q, W_k, W_v$ 的部分：
> - $W_q = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$（单位矩阵），
> - $W_k = \begin{bmatrix} 2 & 0 \\ 0 & 2 \end{bmatrix}$（2 倍单位矩阵），
> - $W_v = \begin{bmatrix} 3 & 0 \\ 0 & 3 \end{bmatrix}$（3 倍单位矩阵）。
>
> #### **计算 $q, k, v$**
> - `qkv = self.c_attn(x)`：
>   - 对于 $x_1 = [1.0, 0.0]$：
>     - $q_1 = W_q \cdot x_1 = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} \cdot \begin{bmatrix} 1.0 \\ 0.0 \end{bmatrix} = \begin{bmatrix} 1.0 \\ 0.0 \end{bmatrix}$，
>     - $k_1 = W_k \cdot x_1 = \begin{bmatrix} 2 & 0 \\ 0 & 2 \end{bmatrix} \cdot \begin{bmatrix} 1.0 \\ 0.0 \end{bmatrix} = \begin{bmatrix} 2.0 \\ 0.0 \end{bmatrix}$，
>     - $v_1 = W_v \cdot x_1 = \begin{bmatrix} 3 & 0 \\ 0 & 3 \end{bmatrix} \cdot \begin{bmatrix} 1.0 \\ 0.0 \end{bmatrix} = \begin{bmatrix} 3.0 \\ 0.0 \end{bmatrix}$。
>   - 对于 $x_2 = [0.0, 1.0]$：
>     - $q_2 = W_q \cdot x_2 = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} \cdot \begin{bmatrix} 0.0 \\ 1.0 \end{bmatrix} = \begin{bmatrix} 0.0 \\ 1.0 \end{bmatrix}$，
>     - $k_2 = W_k \cdot x_2 = \begin{bmatrix} 2 & 0 \\ 0 & 2 \end{bmatrix} \cdot \begin{bmatrix} 0.0 \\ 1.0 \end{bmatrix} = \begin{bmatrix} 0.0 \\ 2.0 \end{bmatrix}$，
>     - $v_2 = W_v \cdot x_2 = \begin{bmatrix} 3 & 0 \\ 0 & 3 \end{bmatrix} \cdot \begin{bmatrix} 0.0 \\ 1.0 \end{bmatrix} = \begin{bmatrix} 0.0 \\ 3.0 \end{bmatrix}$。
> - 因此：
>   - $k = \begin{bmatrix} [2.0, 0.0], [0.0, 2.0] \end{bmatrix}$，
>   - $v = \begin{bmatrix} [3.0, 0.0], [0.0, 3.0] \end{bmatrix}$。
>
> ---
>
> #### **生成 token3**
> 假设生成了 token3，其嵌入 $x_3 = [1.0, 1.0]$（实际中由模型预测，这里为了示例假设一个值）。现在序列变为 `[token1, token2, token3]`，$x = [[1.0, 0.0], [0.0, 1.0], [1.0, 1.0]]$（形状为 $(1, 3, 2)$）。
>
> #### **重新计算 $q, k, v$**
> - 对于 $x_1 = [1.0, 0.0]$：
>   - $k_1 = W_k \cdot x_1 = \begin{bmatrix} 2 & 0 \\ 0 & 2 \end{bmatrix} \cdot \begin{bmatrix} 1.0 \\ 0.0 \end{bmatrix} = \begin{bmatrix} 2.0 \\ 0.0 \end{bmatrix}$，
>   - $v_1 = W_v \cdot x_1 = \begin{bmatrix} 3 & 0 \\ 0 & 3 \end{bmatrix} \cdot \begin{bmatrix} 1.0 \\ 0.0 \end{bmatrix} = \begin{bmatrix} 3.0 \\ 0.0 \end{bmatrix}$。
> - 对于 $x_2 = [0.0, 1.0]$：
>   - $k_2 = W_k \cdot x_2 = \begin{bmatrix} 2 & 0 \\ 0 & 2 \end{bmatrix} \cdot \begin{bmatrix} 0.0 \\ 1.0 \end{bmatrix} = \begin{bmatrix} 0.0 \\ 2.0 \end{bmatrix}$，
>   - $v_2 = W_v \cdot x_2 = \begin{bmatrix} 3 & 0 \\ 0 & 3 \end{bmatrix} \cdot \begin{bmatrix} 0.0 \\ 1.0 \end{bmatrix} = \begin{bmatrix} 0.0 \\ 3.0 \end{bmatrix}$。
> - 对于 $x_3 = [1.0, 1.0]$：
>   - $k_3 = W_k \cdot x_3 = \begin{bmatrix} 2 & 0 \\ 0 & 2 \end{bmatrix} \cdot \begin{bmatrix} 1.0 \\ 1.0 \end{bmatrix} = \begin{bmatrix} 2.0 \\ 2.0 \end{bmatrix}$，
>   - $v_3 = W_v \cdot x_3 = \begin{bmatrix} 3 & 0 \\ 0 & 3 \end{bmatrix} \cdot \begin{bmatrix} 1.0 \\ 1.0 \end{bmatrix} = \begin{bmatrix} 3.0 \\ 3.0 \end{bmatrix}$。
>
> ---
>
> #### **关键观察**
> - 在生成 token3 后：
>   - $k_1$ 仍然是 $[2.0, 0.0]$，$v_1$ 仍然是 $[3.0, 0.0]$，
>   - $k_2$ 仍然是 $[0.0, 2.0]$，$v_2$ 仍然是 $[0.0, 3.0]$，
>   - 只是新增了 $k_3$ 和 $v_3$。
>
> 这表明，之前的 $k_1, v_1, k_2, v_2$ 的值没有改变，因为它们的计算只依赖于 $x_1, x_2$ 和固定的 $W_k, W_v$。
>
> ---
>
> ### 4. 为什么代码中每次都重新计算？
>
> 在标准的 Transformer 实现中，每次生成新 token 时，确实会重新计算整个序列的 $q, k, v$。但正如例子所示，之前的 $k$ 和 $v$ 在数值上与之前相同，因此可以缓存起来，避免重复计算。这正是 KV Cache 的核心思想。
>
> - **数值不变的原因**：
>   - $k_1, v_1$ 只依赖于 $x_1$ 和 $W_k, W_v$，而 $x_1$ 和 $W_k, W_v$ 在生成过程中不变，因此 $k_1, v_1$ 不变。
>   - 同理，$k_2, v_2$ 只依赖于 $x_2$ 和 $W_k, W_v$，因此也不变。
>
> ---
>
> ### 5. 总结
>
> - **权重是固定的**：`self.c_attn` 的权重在整个生成过程中不变，你的理解“每次调用 `model(x)` 时，`c_attn` 的权重应该不一样”是错误的。
> - **输入局部性**：每个 token 的 $k$ 和 $v$ 只依赖于该 token 的嵌入和固定的权重。
> - **数值不变**：对于之前的 token，它们的 $k$ 和 $v$ 在生成新 token 时保持不变。
>
> 希望这个解释和例子能帮助你理解为什么“从数值上看，$k_1, v_1, k_2, v_2$ 的值与之前序列 `[token1, token2]` 时计算的结果是相同的”。如果还有疑问，欢迎继续提问！
>





### MHA、MQA、GQA、MLA区别

#### 1. Multi-Head Attention (MHA)

> **定义：**  
>MHA 是 Transformer 模型中的标准注意力机制，通过多个注意力头并行计算，允许模型从不同的表示子空间捕获信息。每个头都有独立的查询（Query）、键（Key）和值（Value）投影。
> 
>**数学公式：**  
> 给定输入 $X \in \mathbb{R}^{N \times D}$，其中 $N$ 是序列长度，$D$ 是模型维度。MHA 将输入投影到查询、键和值：  
>$$
> Q = X W_Q, \quad K = X W_K, \quad V = X W_V
> $$
>其中 $W_Q, W_K, W_V \in \mathbb{R}^{D \times D}$ 是可学习的投影矩阵。  
> 
> 将投影后的 $Q, K, V$ 分成 $H$ 个头：  
> $$
> Q_h = Q[:, h \cdot D_H : (h+1) \cdot D_H], \quad K_h = K[:, h \cdot D_H : (h+1) \cdot D_H], \quad V_h = V[:, h \cdot D_H : (h+1) \cdot D_H]
> $$
> 其中 $D_H = D / H$ 是每个头的维度。  
>
> 每个头的注意力分数计算为：  
> $$
> A_h = \text{softmax}\left( \frac{Q_h K_h^T}{\sqrt{D_H}} \right)
> $$
> 
>头的输出为：  
> $$
> O_h = A_h V_h
> $$
> 
>最后，所有头的输出被拼接并通过一个线性层：  
> $$
> O = \text{concat}(O_1, O_2, \dots, O_H) W_O
> $$
> 其中 $W_O \in \mathbb{R}^{D \times D}$。
>
> **伪代码：**  
> ```python
> def multi_head_attention(X, W_Q, W_K, W_V, W_O, H):
>  Q = X @ W_Q
>  K = X @ W_K
> V = X @ W_V
>  D_H = D // H
>  Q_heads = [Q[:, h*D_H:(h+1)*D_H] for h in range(H)]
>  K_heads = [K[:, h*D_H:(h+1)*D_H] for h in range(H)]
>     V_heads = [V[:, h*D_H:(h+1)*D_H] for h in range(H)]
>     O_heads = []
>     for h in range(H):
>         A_h = softmax(Q_heads[h] @ K_heads[h].T / sqrt(D_H))
>         O_h = A_h @ V_heads[h]
>         O_heads.append(O_h)
>     O = concat(O_heads, axis=1) @ W_O
>     return O
>    ```
>    

#### 2. Multi-Query Attention (MQA)

> **定义：**  
> MQA 是一种注意力变体，其中查询 $Q$ 在所有头中共享，而键 $K$ 和值 $V$ 在每个头中独立计算。这种设计可以减少计算量，特别是在处理长序列时。
>
> **数学公式：**  
> 查询 $Q$ 是共享的：  
> $$
> Q = X W_Q
> $$
> 其中 $W_Q \in \mathbb{R}^{D \times D_Q}$，$D_Q$ 是查询的维度。  
>
> 对于每个头 $h$，键和值有自己的投影矩阵：  
> $$
> K_h = X W_{K_h}, \quad V_h = X W_{V_h}
> $$
> 其中 $W_{K_h}, W_{V_h} \in \mathbb{R}^{D \times D_H}$。  
>
> 注意力分数计算为：  
> $$
> A_h = \text{softmax}\left( \frac{Q K_h^T}{\sqrt{D_H}} \right)
> $$
>
> 头的输出为：  
> $$
> O_h = A_h V_h
> $$
>
> 最后，输出被拼接并通过线性层：  
> $$
> O = \text{concat}(O_1, O_2, \dots, O_H) W_O
> $$
>
> **伪代码：**  
>
> ```python
> def multi_query_attention(X, W_Q, W_Ks, W_Vs, W_O, H):
>  Q = X @ W_Q  # Shared query
>  D_H = D // H
>  K_heads = [X @ W_Ks[h] for h in range(H)]
>  V_heads = [X @ W_Vs[h] for h in range(H)]
>  O_heads = []
>  for h in range(H):
>      A_h = softmax(Q @ K_heads[h].T / sqrt(D_H))
>      O_h = A_h @ V_heads[h]
>      O_heads.append(O_h)
>  O = concat(O_heads, axis=1) @ W_O
>  return O
> ```
>

#### 3. Grouped Query Attention (GQA)

> **定义：**  
> GQA 是 MQA 的扩展，将查询分组，每个组共享一组键和值。这种机制可以在保持性能的同时进一步减少计算复杂度。
>
> **数学公式：**  
> 假设将 $H$ 个头分为 $G$ 组，每组有 $H_G = H / G$ 个头。  
>
> 对于每个组 $g$，有共享的查询 $Q_g$：  
> $$
> Q_g = X W_{Q_g}
> $$
> 其中 $W_{Q_g} \in \mathbb{R}^{D \times D_Q}$。  
>
> 对于每个头 $h$ 在组 $g$ 中，键和值是共享的：  
> $$
> K_{g} = X W_{K_g}, \quad V_{g} = X W_{V_g}
> $$
>
> 注意力分数为：  
> $$
> A_{h,g} = \text{softmax}\left( \frac{Q_g K_{g}^T}{\sqrt{D_H}} \right)
> $$
>
> 头的输出为：  
> $$
> O_{h,g} = A_{h,g} V_{g}
> $$
>
> 最后，输出被拼接并通过线性层：  
> $$
> O = \text{concat}(O_{1,1}, \dots, O_{H_G,G}) W_O
> $$
>
> **伪代码：**  
>
> ```python
> def grouped_query_attention(X, W_Qs, W_Ks, W_Vs, W_O, H, G):
>  D_H = D // H
>  O_heads = []
>  for g in range(G):
>      Q_g = X @ W_Qs[g]
>      K_g = X @ W_Ks[g]
>      V_g = X @ W_Vs[g]
>      for h in range(H // G):
>          A_hg = softmax(Q_g @ K_g.T / sqrt(D_H))
>          O_hg = A_hg @ V_g
>          O_heads.append(O_hg)
>  O = concat(O_heads, axis=1) @ W_O
>  return O
> ```
>
> ### 三种注意力机制的区别
>
> - **MHA (Multi-Head Attention)**：  
>   - 每个头有独立的查询、键和值投影。  
>   - 计算复杂度较高，适用于需要捕捉多种表示子空间的场景。  
> - **MQA (Multi-Query Attention)**：  
>   - 查询在所有头中共享，键和值在每个头中独立。  
>   - 减少了查询的计算量，适用于长序列处理。  
> - **GQA (Grouped Query Attention)**：  
>   - 查询被分组，每组共享一组键和值。  
>   - 进一步减少计算复杂度，同时保持一定的表达能力。  

#### 4. Multi-head Latent Attention(MLA)

