大模型性能评测之大海捞针(Needle In A Haystack)

## 1. 导入

大模型在卷上下文长度context length，那对于长文本的处理，大模型的性能如何呢？又应该如何评测呢？

gkamradt的一项[极限测试](https://github.com/gkamradt/LLMTest_NeedleInAHaystack/tree/main)却发现，大部分人用法都不对，没发挥出AI应有的实力。

AI真的能从几十万字中找到特定关键事实吗？颜色越红代表AI犯的错越多。

![alt text](assest/大模型性能评测之大海捞针/0.png)

gkamradt将这项测试命名为NeedleInAHaystack[草垛找针]，中文翻译为大海捞针，是一种评估大模型长文本性能的方法。

简而言之就是把一个关键信息（针）藏在一个长文本Prompt（草垛/大海）中，然后通过提问让大模型找到这个关键信息。

由于这个测试确实能反映出大模型的能力，现在已经逐渐发展为一种标准的评估方法。


## 2.大海捞针任务简述
Kamradt把藏起来的那句话（也就是大海捞针的“针”）分别放到了文本语料（也就是大海捞针的“大海”）从前到后的15处不同位置，然后针对从1K到128K（200K）等量分布的15种不同长度的语料进行了225 次（15×15）实验。

Greg Kamradt 的“大海捞针”实验简述：

**大海**
```Plain Text
YC创始人Paul Graham的218篇博客文章
```

**针**
```Plain Text
The best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny day.
在旧金山最好的事情，就是在阳光明媚的日子坐在多洛雷斯公园吃一个三明治.
```

**提问**

```Plain Text
What is the most fun thing to do in San Francisco based on my context? Don't give information outside the document
```

**期望的回答**

```Plain Text
The best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny day.
```


## 3. 其它大海捞针方法（OpenCompass）

- 单一信息检索任务(Single-Needle Retrieval Task, S-RT)：评估LLM在长文本中提取单一关键信息的能力，测试其对广泛叙述中特定细节的精确回忆能力。这对应于原始的大海捞针测试任务设定。

- 多信息检索任务(Multi-Needle Retrieval Task, M-RT)：探讨LLM从长文本中检索多个相关信息的能力，模拟实际场景中对综合文档的复杂查询。

- 多信息推理任务(Multi-Needle Reasoning Task, M-RS)：通过提取并利用长文本中的多个关键信息来评估LLM的长文本能力，要求模型对各关键信息片段有综合理解。

- 祖先追溯挑战(Ancestral Trace Challenge, ATC)：通过设计“亲属关系针”，测试LLM处理真实长文本中多层逻辑挑战的能力。在ATC任务中，通过一系列逻辑推理问题，检验模型对长文本中每个细节的记忆和分析能力，在此任务中，我们去掉了无关文本(Haystack)的设定，而是将所有文本设计为关键信息，LLM必须综合运用长文本中的所有内容和推理才能准确回答问题。

后面，我们再来看***数星星***的评测方案。

## 参考

<div id="refer-anchor-1"></div>

[1] [大海捞针(Needle In A Haystack)实验评估](https://opencompass.readthedocs.io/zh-cn/latest/advanced_guides/needleinahaystack_eval.html)

[2] [AI大模型测评报告：“长文本”和“捞针”成大模型痛点](https://finance.eastmoney.com/a/202407033120965459.html)

## 欢迎关注我的GitHub和微信公众号，来不及解释了，快上船！

[GitHub: LLMForEverybody](https://github.com/luhengshiwo/LLMForEverybody)

仓库上有原始的Markdown文件，完全开源，欢迎大家Star和Fork！