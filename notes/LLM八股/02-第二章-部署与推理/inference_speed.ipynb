{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##计算参数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeb0391a830341b6a6592a641e5b1d12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/681 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2Model(\n",
      "  (embed_tokens): Embedding(151936, 896)\n",
      "  (layers): ModuleList(\n",
      "    (0-23): 24 x Qwen2DecoderLayer(\n",
      "      (self_attn): Qwen2Attention(\n",
      "        (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
      "        (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "        (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "        (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
      "      )\n",
      "      (mlp): Qwen2MLP(\n",
      "        (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "        (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "        (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
      "        (act_fn): SiLU()\n",
      "      )\n",
      "      (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "      (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "    )\n",
      "  )\n",
      "  (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "  (rotary_emb): Qwen2RotaryEmbedding()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoModel\n",
    "\n",
    "# 选择模型名称，例如 \"bert-base-uncased\"\n",
    "model_name = \"Qwen/Qwen2.5-0.5B\"\n",
    "\n",
    "\n",
    "# 加载模型配置\n",
    "config = AutoConfig.from_pretrained(model_name,trust_remote_code=True)\n",
    "\n",
    "# # 使用配置创建模型\n",
    "model = AutoModel.from_config(config)\n",
    "\n",
    "# 打印模型架构\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参数量计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总参数量: 31.98 B\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "  \"attention_dropout\": 0.0,\n",
    "  \"bos_token_id\": 151643,\n",
    "  \"eos_token_id\": 151643,\n",
    "  \"hidden_act\": \"silu\",\n",
    "  \"hidden_size\": 5120,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"intermediate_size\": 27648,\n",
    "  \"max_position_embeddings\": 131072,\n",
    "  \"max_window_layers\": 64,\n",
    "  \"model_type\": \"qwen2\",\n",
    "  \"num_attention_heads\": 40,\n",
    "  \"num_hidden_layers\": 64,\n",
    "  \"num_key_value_heads\": 8,\n",
    "  \"rms_norm_eps\": 1e-05,\n",
    "  \"rope_theta\": 1000000.0,\n",
    "  \"sliding_window\": 131072,\n",
    "  \"torch_dtype\": \"bfloat16\",\n",
    "  \"transformers_version\": \"4.43.1\",\n",
    "  \"vocab_size\": 152064\n",
    "}\n",
    "\n",
    "\n",
    "def calculate_total_parameters(config):\n",
    "    # 计算嵌入层参数量\n",
    "    embedding_params = config['vocab_size'] * config['hidden_size']\n",
    "    # 计算每层的参数量\n",
    "    # 前馈网络（FFN）部分\n",
    "    ffn_params = 3 * (config['hidden_size'] * config['intermediate_size'])  # 三个线性层\n",
    "    # 多头注意力机制部分 Q, K, V\n",
    "    attention_params = 2 * config['hidden_size'] * config['hidden_size']*config['num_key_value_heads']/config['num_attention_heads'] + config['hidden_size'] * config['hidden_size']\n",
    "    # 输出投影部分O\n",
    "    output_projection_params = config['hidden_size'] * config['hidden_size']  # 输出投影\n",
    "    # 每层的总参数量\n",
    "    layer_params = ffn_params + attention_params + output_projection_params\n",
    "    # 总参数量\n",
    "    total_params = embedding_params + layer_params * config['num_hidden_layers']\n",
    "    return total_params/ 1e9\n",
    "    \n",
    "    \n",
    " # 总参数量\n",
    "total_params = calculate_total_parameters(config)\n",
    "print(f\"总参数量: {total_params:.2f} B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prefilling阶段计算量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefilling阶段总计算量: 65.28 TFLOPs\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "  \"attention_dropout\": 0.0,\n",
    "  \"bos_token_id\": 151643,\n",
    "  \"eos_token_id\": 151643,\n",
    "  \"hidden_act\": \"silu\",\n",
    "  \"hidden_size\": 5120,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"intermediate_size\": 27648,\n",
    "  \"max_position_embeddings\": 131072,\n",
    "  \"max_window_layers\": 64,\n",
    "  \"model_type\": \"qwen2\",\n",
    "  \"num_attention_heads\": 40,\n",
    "  \"num_hidden_layers\": 64,\n",
    "  \"num_key_value_heads\": 8,\n",
    "  \"rms_norm_eps\": 1e-05,\n",
    "  \"rope_theta\": 1000000.0,\n",
    "  \"sliding_window\": 131072,\n",
    "  \"torch_dtype\": \"bfloat16\",\n",
    "  \"transformers_version\": \"4.43.1\",\n",
    "  \"vocab_size\": 152064,\n",
    "  \"prompt_token_length\": 1024,\n",
    "  \"output_token_length\": 1024,\n",
    "  \"batch_size\": 1,\n",
    "}\n",
    "\n",
    "def calculate_prefilling_FLOPs(config):\n",
    "    ## Q投影计算量\n",
    "    query_projection_flops = 2*config['prompt_token_length'] * config['hidden_size']**2 \n",
    "    ## K,v投影计算量\n",
    "    key_projection_flops = 2* config['prompt_token_length'] * config['hidden_size']**2 * config['num_key_value_heads']/config['num_attention_heads'] \n",
    "    value_projection_flops = 2* config['prompt_token_length'] * config['hidden_size']**2 * config['num_key_value_heads']/config['num_attention_heads'] \n",
    "    ## attention计算量\n",
    "    # kv 在GQA的状态下，kv的存储量变小，但是计算量不变，因为K和V会有广播\n",
    "    Q_K_flops = 2* config['prompt_token_length']**2 * config['hidden_size']\n",
    "    A_V_flops = 2* config['prompt_token_length']**2 * config['hidden_size']\n",
    "    ## 输出投影计算量\n",
    "    output_projection_flops = 2*config['prompt_token_length'] * config['hidden_size']**2 \n",
    "\n",
    "    ## 前馈网络计算量\n",
    "    ## swiGLu 有三次线性变换\n",
    "    ffn_flops = 3* 2* config['prompt_token_length'] * config['hidden_size'] * config['intermediate_size']\n",
    "\n",
    "    layer_flops = query_projection_flops + key_projection_flops + value_projection_flops + Q_K_flops + A_V_flops + output_projection_flops + ffn_flops\n",
    "\n",
    "    total_flops = layer_flops * config['num_hidden_layers']*config['batch_size']\n",
    "    return total_flops/ 1e12\n",
    "    \n",
    "    \n",
    "calculate_prefilling_FLOPs(config)\n",
    " \n",
    " \n",
    "def calculate_prefilling_FLOPs_quick(config):\n",
    "    total_flops = (4*(1+config['num_key_value_heads']/config['num_attention_heads'])*config['prompt_token_length']*config['hidden_size']**2\n",
    "                   + 4*config['prompt_token_length']**2*config['hidden_size']\n",
    "                   + 6*config['prompt_token_length'] *\n",
    "                   config['hidden_size']*config['intermediate_size'])*config['num_hidden_layers']*config['batch_size']\n",
    "    return total_flops/ 1e12\n",
    "    \n",
    "    \n",
    "calculate_prefilling_FLOPs_quick(config)\n",
    "\n",
    "\n",
    "total_prefilling_flops = calculate_prefilling_FLOPs(config)\n",
    "print(f\"Prefilling阶段总计算量: {total_prefilling_flops:.2f} TFLOPs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding阶段计算量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第10个token的计算量: 0.06 TFLOPs\n"
     ]
    }
   ],
   "source": [
    "def calculate_decoding_FLOPs_onestep(config,step):\n",
    "    ## Q投影计算量\n",
    "    query_projection_flops = 2* config['hidden_size']**2 \n",
    "    ## K,v投影计算量，每次计算一个token的kv\n",
    "    key_projection_flops = 2* config['hidden_size']**2 * config['num_key_value_heads']/config['num_attention_heads'] \n",
    "    value_projection_flops = 2* config['hidden_size']**2 * config['num_key_value_heads']/config['num_attention_heads'] \n",
    "    ## attention计算量\n",
    "    # kv cache的状态下，KV的大小的随着step的增加而增加，从初始的prompt_token_length 到最终的prompt_token_length+output_token_length\n",
    "    Q_K_flops = 2* (config['prompt_token_length']+step) * config['hidden_size']\n",
    "    A_V_flops = 2* (config['prompt_token_length']+step) * config['hidden_size']\n",
    "    ## 输出投影计算量\n",
    "    output_projection_flops = 2* config['hidden_size']**2 \n",
    "\n",
    "    ## 前馈网络计算量\n",
    "    ## swiGLu 有三次线性变换\n",
    "    ffn_flops = 3* 2* config['hidden_size'] * config['intermediate_size']\n",
    "\n",
    "    layer_flops = query_projection_flops + key_projection_flops + value_projection_flops + Q_K_flops + A_V_flops + output_projection_flops + ffn_flops\n",
    "\n",
    "    total_flops = layer_flops * config['num_hidden_layers']*config['batch_size']\n",
    "    return total_flops/ 1e12\n",
    "\n",
    "step = 10\n",
    "decoding_FLOPs_per_token = calculate_decoding_FLOPs_onestep(config,step)\n",
    "print(f\"第{step}个token的计算量: {decoding_FLOPs_per_token:.2f} TFLOPs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_decoding_FLOPs_per_token(config):\n",
    "        ## Q投影计算量\n",
    "    query_projection_flops = 2* config['hidden_size']**2 \n",
    "    ## K,v投影计算量，每次计算一个token的kv\n",
    "    key_projection_flops = 2* config['hidden_size']**2 * config['num_key_value_heads']/config['num_attention_heads'] \n",
    "    value_projection_flops = 2* config['hidden_size']**2 * config['num_key_value_heads']/config['num_attention_heads'] \n",
    "    ## attention计算量\n",
    "    # kv cache的状态下，KV的大小的随着step的增加而增加，从初始的prompt_token_length 到最终的prompt_token_length+output_token_length\n",
    "    Q_K_flops = 2* (config['prompt_token_length']+(1+config['output_token_length'])/2) * config['hidden_size']\n",
    "    A_V_flops = 2* (config['prompt_token_length']+(1+config['output_token_length'])/2) * config['hidden_size']\n",
    "    ## 输出投影计算量\n",
    "    output_projection_flops = 2* config['hidden_size']**2 \n",
    "\n",
    "    ## 前馈网络计算量\n",
    "    ## swiGLu 有三次线性变换\n",
    "    ffn_flops = 3* 2* config['hidden_size'] * config['intermediate_size']\n",
    "\n",
    "    layer_flops = query_projection_flops + key_projection_flops + value_projection_flops + Q_K_flops + A_V_flops + output_projection_flops + ffn_flops\n",
    "\n",
    "    total_flops = layer_flops * config['num_hidden_layers']*config['batch_size']\n",
    "    return total_flops/ 1e12\n",
    "    \n",
    "decoding_FLOPs_per_token = calculate_decoding_FLOPs_per_token(config)\n",
    "print(f\"平均每个token的计算量: {decoding_FLOPs_per_token:.2f} TFLOPs\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "handson",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
