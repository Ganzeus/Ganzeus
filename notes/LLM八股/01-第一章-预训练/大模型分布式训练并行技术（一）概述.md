大模型分布式训练并行技术（一）综述

## 引言

>为了训练最大的 Llama 3 模型，Meta 结合了三种并行化方式：数据并行化、模型并行化和管道并行化。当同时在 16K GPU 上进行训练时，他们最高效的实现实现了每 GPU 超过 400 TFLOPS 的计算利用率。他们在两个定制的 24K GPU 集群上进行了训练运行。为了最大限度地延长 GPU 的正常运行时间，他们开发了一种新的训练堆栈，可以自动检测、处理和维护错误。他们还大大改进了硬件可靠性和无声数据损坏检测机制，并开发了新的可扩展存储系统，减少了检查点和回滚的开销。这些改进使总体有效训练时间缩短了 95% 以上，与 Llama 2 相比，将 Llama 3 的训练效率提高了约三倍。

![alt text](assest/大模型分布式训练并行技术（一）概述/00.png)

这边，meta使用了三种并行化方式：数据并行化、模型并行化和管道并行化。这三种并行化方式是大模型分布式训练的核心技术。在这个系列我将对这几种并行化方式进行详细介绍。

本系列旨在用最简单的方式介绍大模型分布式训练并行技术，不涉及太多细节和实现方式，只介绍最基本的概念。

## 并行化策略

1. 数据并行 DataParallel  (DP) - 相同的设置被复制多次，每次都输入一部分数据。处理是并行进行的，所有设置在每个训练步骤结束时同步。

![alt text](assest/大模型分布式训练并行技术（一）概述/0.png)

2. 张量并行 TensorParallel (TP) - 每个张量被分成多个块，因此不是将整个张量驻留在单个 gpu 上，而是将张量的每个分片驻留在其指定的 gpu 上。在处理过程中，每个分片在不同的 GPU 上单独并行处理，结果在步骤结束时同步。这就是所谓的水平并行，因为拆分发生在水平层面。

![alt text](assest/大模型分布式训练并行技术（一）概述/2.png)

3. 管道并行（流水线并行） PipelineParallel (PP) - 模型在多个 GPU 上垂直（层级）拆分，因此只有一层或几层模型放在单个 gpu 上。每个 gpu 并行处理管道的不同阶段并处理一小部分批次。

![alt text](assest/大模型分布式训练并行技术（一）概述/4.png)

4. 混合并行 MixedParallel (MP) - 混合并行是将数据并行、模型并行和管道并行结合在一起的一种方法。这种方法可以在多个 gpu 上同时进行数据并行和模型并行，同时在每个 gpu 上进行管道并行。

我打算分几篇文章来介绍这几种并行化方式，下一篇是数据并行。

## 参考

<div id="refer-anchor-1"></div>

[1] [刚刚，全球最强开源大模型 Llama 3 发布：使用 15T 数据预训练，最大模型参数将超 4000 亿](https://www.163.com/dy/article/J03PMO8I0531E3NX.html)

[2] [Model Parallelism](https://huggingface.co/docs/transformers/v4.15.0/en/parallelism)

## 欢迎关注我的GitHub和微信公众号，来不及解释了，快上船！

[GitHub: LLMForEverybody](https://github.com/luhengshiwo/LLMForEverybody)

仓库上有原始的Markdown文件，完全开源，欢迎大家Star和Fork！