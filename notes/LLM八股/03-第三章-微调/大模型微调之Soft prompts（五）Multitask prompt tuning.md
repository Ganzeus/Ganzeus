LoRA不是唯一选择，Soft Prompts微调大模型的奥秘（五）Multitask prompt tuning

2023年5月，时隔2年后，soft-prompts又有了新方法，我第一次看到这个方法的时候，心里闪过一个词：一顿操作猛如虎。我的表情就是这样的：

![alt text](<assest/大模型微调之Soft prompts（五）Multitask prompt tuning/1.png>)

要不是为了这个系列的完备性，也许我永远不会看这篇paper！我建议大家直接看直观解析这一节。

## 技术解读

![alt text](<assest/大模型微调之Soft prompts（五）Multitask prompt tuning/0.png>)

多任务提示调整 (MPT) 从多种任务类型的数据中学习单个提示，这些提示可共享给不同的目标任务。其他现有方法为每个需要检索或聚合以适应目标任务的任务学习单独的软提示。MPT 包含两个阶段：

1. 源训练 - 对于每个任务，其软提示被分解为特定于任务的向量。特定于任务的向量相乘以形成另一个矩阵 W，并在 W 和共享提示矩阵 P 之间使用 Hadamard 积来生成特定于任务的提示矩阵。特定于任务的提示被提炼为在所有任务之间共享的单个提示矩阵。此提示通过多任务训练进行训练。

2. 目标适应 - 为了使单个提示适应目标任务，目标提示被初始化并表示为共享提示矩阵和特定于任务的低秩提示矩阵的 Hadamard 积（逐个元素点积）。

## 直观解释

想象一下，有一个学生要准备参加好几门不同的考试，比如数学、历史和科学。在MPT中，这个学生就像是预训练的语言模型，而考试则像是不同的下游任务。

1. 学习通用知识点：在学期开始时，老师会给学生一些通用的知识点，这些知识点在所有课程中都是有用的，比如解题技巧、写作方法或者分析框架。这就像是在MPT中，模型学习一个通用的提示，这个提示可以帮助它理解不同类型的任务。

2. 针对不同课程复习：然后，针对每门课程的考试，老师可能会给学生一些特定的复习指导或者重点，这些是根据每门课程的特点来定制的。在MPT中，这意味着模型会根据每个任务的特点对通用提示进行一些微调，以便更好地适应每个任务。

3. 考试时应用：考试时，学生利用已经学到的通用知识点和针对每门课程的复习重点来答题。在MPT中，模型在面对新任务时，会使用已经学到的通用提示，并根据任务的需要进行调整，以产生正确的输出。

## 参考

<div id="refer-anchor-1"></div>

[1] [Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning](https://arxiv.org/abs/2303.02861)

## 欢迎关注我的GitHub和微信公众号[真-忒修斯之船]，来不及解释了，快上船！

[GitHub: LLMForEverybody](https://github.com/luhengshiwo/LLMForEverybody)

仓库上有原始的Markdown文件，完全开源，欢迎大家Star和Fork！